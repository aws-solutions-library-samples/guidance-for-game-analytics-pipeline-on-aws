{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Made by Daniel Lee, Nathan Yee, Matthew Kwan, Christian Orellana, Rene Roldan, and Steve Parker","text":""},{"location":"index.html#special-thanks-to-andrei-muratov-reed-froehlich-narendra-gupta-and-satesh-sonti","title":"Special thanks to Andrei Muratov, Reed Froehlich, Narendra Gupta and Satesh Sonti","text":""},{"location":"index.html#overview","title":"Overview","text":"<p>The games industry is increasing adoption of the Games-as-a-Service operating model, where games have become more like a service than a product, and recurring revenue is frequently generated through in-app purchases, subscriptions, and other techniques. With this change, it is critical to develop a deeper understanding of how players use the features of games and related services. This understanding allows game developers to continually adapt, and make the necessary changes to keep players engaged.</p> <p>The Game Analytics Pipeline guidance helps game developers to apply a flexible, and scalable DataOps methodology to their games. Allowing them to continuously integrate, and continuously deploy (CI/CD) a scalable serverless data pipeline for ingesting, storing, and analyzing telemetry data generated from games, and services. The guidance supports streaming ingestion of data, allowing users to gain critical insights from their games, and other applications in near real-time, allowing them to focus on expanding, and improving game experience almost immediately, instead of managing the underlying infrastructure operations. Since the guidance has been codified as a CDK application, game developers can determine the best modules, or components that fit their use case, allowing them to test, and QA the best architecture before deploying into production. This modular system allows for additional AWS capabilities, such as AI/ML models, to be integrated into the architecture in order to further support real-time decision making, and automated LiveOps using AIOps, to further enhance player engagement. Essentially allowing developers to focus on expanding game functionality, rather than managing the underlying infrastructure operations.</p>"},{"location":"component-deep-dive.html","title":"Component Deep Dive","text":""},{"location":"component-deep-dive.html#overview","title":"Overview","text":"<p>The Game Analytics Pipeline Guidance has the following modes:</p> <ol> <li><code>DATA_LAKE</code> - Deploys a lightweight data lake cost-optimized for lower data scan volume and ad-hoc queries</li> </ol> <p></p> <ol> <li><code>REDSHIFT</code> - Deploys a serverless Redshift Data Warehouse cost-optimized for larger data scan volume and more frequent queries</li> </ol> <p></p> <p>From there the Guidance allows several ingest options. Some ingest options are required for certain modes, refer to Configurations Reference Page for a full list:</p> <ol> <li> <p><code>DIRECT_BATCH</code> - Does not deploy the real-time infrastructure components. Sends directly via batch in near-real-time to Firehose for <code>DATA_LAKE</code> mode</p> </li> <li> <p><code>KINESIS_DATA_STREAMS</code> - Deploys additional real-time infrastructure components for real-time analysis</p> </li> </ol> <p>Optionally, there is a real-time analytics mode for time-sensitive analytics:</p> <p></p> <p>For help deciding between the modes and options, or even for explanations and justifications for why we chose the services and processes below, refer to the Design Considerations Page</p>"},{"location":"component-deep-dive.html#1-source","title":"1. Source","text":"<p>The Game Analytics Pipeline Guidance can accept from any HTTP/HTTPS REST supported sources, such as Game Clients, Game Servers, or Backend services. Refer to the API Reference Page and Getting Started Guide on how to send events to the endpoint.</p> <p></p>"},{"location":"component-deep-dive.html#2-endpoint","title":"2. Endpoint","text":"<ol> <li> <p>API Gateway hosts a managed REST API endpoint configured to either:</p> <ul> <li>Send to the analytics ingest infrastructure based on the above configurations in the overview section</li> <li>Perform administrative tasks</li> </ul> </li> <li> <p>Two helper DynamoDB tables hold the following:</p> <ul> <li><code>Applications Table</code>: Holds Application IDs which represent a specific game/application to perform per-application analytics on. Applications can be created or deleted through Administrative API calls</li> <li><code>Authorizations Table</code>: Holds API authorization tokens for each Application used to authorize sending events to the Application (like a password). When sending events, the API Key's value/code is included in the <code>Authorization</code> header for security. Authorizations can be created or deleted through Administrative API calls</li> </ul> </li> <li> <p>Events sent through REST API will first go through the integrated Lambda Authorizer, which dissects the API call's headers, checks the DynamoDB authorizer table entries against the one sent from the event, and checks if the caller's IAM passed through SigV4 allows for sending events. If everything passes, API Gateway proceeds with executing the API call.</p> </li> <li> <p>Based on the guidance configurations, API Gateway performs the following:</p> <ul> <li><code>KINESIS_DATA_STREAMS</code> - Sends a passthrough call directly to Amazon Kinesis Data Streams via stream in real-time</li> <li><code>DIRECT_BATCH</code> - Sends a passthrough call directly to the Amazon services via batch in near-real-time to Firehose for <code>DATA_LAKE</code> mode</li> </ul> </li> </ol> <p></p>"},{"location":"component-deep-dive.html#3-real-time-optional","title":"3. Real-Time (Optional)","text":"<ol> <li> <p>If Real-Time is enabled with Kinesis Data Streams, all incoming events from all clients will have their data ingested into Kinesis Data Streams. Kinesis Data Streams will send multiple outputs, one to the Data Lake / Redshift store for long term analytics, and the other to Managed Flink for real-time ETL</p> </li> <li> <p>Managed Flink performs SQL based queries on time windows of the incoming streaming data, sending the query outputs to OpenSearch service</p> </li> <li> <p>Kinesis Data Streams ingests from Managed Flink to OpenSearch, which ingests the query outputs and the integrated Kibana dashboard can be accessed by users to view created widgets that display graphs and information in real-time</p> </li> </ol> <p></p>"},{"location":"component-deep-dive.html#4-data-platform","title":"4. Data Platform","text":"<p>If <code>KINESIS_DATA_STREAMS</code> is enabled, events come from the respective streaming service.</p> <p>If <code>DIRECT_BATCH</code> is enabled, events come directly from API Gateway.</p> Data Lake ModeRedshift Mode <p></p> <ol> <li> <p>Amazon Data Firehose performs the following actions on the incoming events:</p> <ul> <li>Provides an ingest buffer on incoming data, holding events until it reaches a certain size or after certain time passes</li> <li>Triggers a Lambda Function through its integrated Lambda transformation feature which performs the following:<ul> <li>Validates the event's json format against the AJV2020 standard, a valid Application ID, and the guidance's game event schema set in <code>business-logic/events-processing/config</code></li> <li>Marks the events with a processing timestamp</li> <li>Passes the data to a corresponding folder in S3 (prefix is set in config, default is <code>processed_events</code>, or if not valid, is still sent to not be lost and sent to a <code>firehose-errors/!{firehose:error-output-type}/</code> folder)</li> </ul> </li> <li>If the guidance is set to HIVE tables through the config setting <code>ENABLE_APACHE_ICEBERG_SUPPORT false</code> (default), Firehose also performs in-service partitioning on the data based on date (year, month, day), which represents as nested folders in the S3 data store as a SNAPPY parquet format</li> <li>If the guidance is set to APACHE ICEBERG tables through the config setting <code>ENABLE_APACHE_ICEBERG_SUPPORT true</code>, there is no partitioning needed due to how it partitions under the hood</li> </ul> </li> </ol> <p> 2. Glue Data Catalog is a centralized metadata repository for the events in the Game Analytics Pipeline. An initial catalog is created when deploying the guidance, but gets updated over time through a continuous schema discovery process through the deployed Glue WorkFlow below</p> <p> 3. S3 is the central object storage service that holds all event data and acts as a central Data Lake store, which should have <code>analyticsbucket</code> in the name when deployed from the guidance. Folder structure for reference:</p> <ul> <li><code>raw_events</code> - Can be changed from the config value <code>RAW_EVENTS_PREFIX</code>, holds unprocessed data from Firehose</li> </ul> <p>Folders that will be explained in below sections will also appear, but are shown here for reference:</p> <ul> <li><code>processed_events</code> - Can be changed from the config value <code>PROCESSED_EVENTS_PREFIX</code>, holds processed data from Glue ETL jobs</li> <li><code>athena_query_results</code> - Holds logs and results from Athena Queries performed on the bucket</li> <li><code>glue-scripts</code> - Holds Spark-based ETL scripts used by Glue and modifiable by users to perform ETL</li> <li><code>glueetl-tmp</code> - Temporary folder for holding ephemeral data transformation when Glue is performing ETL</li> </ul> <p> 4. Glue Workflow is an orchestration feature in AWS Glue. Glue Workflow triggers a series of steps either on demand (default), or if configured during setup, on a regular cron schedule. Glue Workflow performs the following:</p> <ul> <li>Triggers a Glue ETL Job on Spark which only processes data since the last invoked job through a bookmark and outputs the data in parquet with an additional <code>application_id</code> partition key. There are no existing transformations in the script, instead focusing on providing a skeleton script with Glue and Spark best practices</li> <li>Triggers a Glue Crawler to update the Glue Data Catalog with any schema updates or changes through crawling the events and their schemas</li> </ul> <p> 5. Athena is a serverless analytics query service that can perform ad-hoc queries and connect to analytics dashboards to use the queries to power visualizations. The guidance provides sample queries for common game use cases (see Customizations for more details) along with sample operational CTAS (Create-Table-as-Select) queries that can also perform certain ad-hoc ETL.</p> <p> 6. Amazon QuickSight or other dashboard technologies can connect to Athena through their plugins, connectors, or direct integration. Dashboards will call Athena to perform the queries that power visualizations and provide insights to users. Dashboard integration is planned for future releases, and does not come directly with the guidance at this time. Refer to Customizations</p> <p></p> <ol> <li> <p>An Amazon Redshift Serverless cluster is deployed. Redshift is initially not integrated with the Kinesis Data Stream until the <code>setup/redshift</code> API call - see API Reference for POST - Setup Redshift, in which a materialized view will be created, as well as a set of other views representing pre-made queries to get you started.</p> </li> <li> <p>You can query data immediately using Amazon Redshift Query Editor in the AWS Console, or connect up other visualization tools compatible with Amazon Redshift. Dashboard integration is planned for future releases, and does not come directly with the guidance at this time. Refer to Customizations</p> </li> </ol> <p>Note</p> <p>By default, the cluster is configured with 4 RPU Compute Capacity, and is accessible on port 5439. Both can be configured in the redshift-construct source for your chosen Infrastructure as Code language in the respective Redshift Construct files.</p>"},{"location":"component-deep-dive.html#administration","title":"Administration","text":"<ol> <li> <p>Users can administer changes to Application IDs or authorization tokens for the Application IDs through API Gateway. See the API Reference for more details.</p> </li> <li> <p>The guidance also provides an operational CloudWatch dashboard to view infrastructure health and metrics, see the Operational Dashboard Reference</p> </li> </ol>"},{"location":"contributing.html","title":"Contributing Guidelines","text":"<p>Thank you for your interest in contributing to our project. Whether it's a bug report, new feature, correction, or additional documentation, we greatly value feedback and contributions from our community.</p> <p>Please read through this document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your bug report or contribution.</p>"},{"location":"contributing.html#reporting-bugsfeature-requests","title":"Reporting Bugs/Feature Requests","text":"<p>We welcome you to use the GitHub issue tracker to report bugs or suggest features.</p> <p>When filing an issue, please check existing open, or recently closed, issues to make sure somebody else hasn't already reported the issue. Please try to include as much information as you can. Details like these are incredibly useful:</p> <ul> <li>A reproducible test case or series of steps</li> <li>The version of our code being used</li> <li>Any modifications you've made relevant to the bug</li> <li>Anything unusual about your environment or deployment</li> </ul>"},{"location":"contributing.html#contributing-via-pull-requests","title":"Contributing via Pull Requests","text":"<p>Contributions via pull requests are much appreciated. Before sending us a pull request, please ensure that:</p> <ol> <li>You are working against the latest source on the main branch.</li> <li>You check existing open, and recently merged, pull requests to make sure someone else hasn't addressed the problem already.</li> <li>You open an issue to discuss any significant work - we would hate for your time to be wasted.</li> </ol> <p>To send us a pull request, please:</p> <ol> <li>Fork the repository.</li> <li>Modify the source; please focus on the specific change you are contributing. If you also reformat all the code, it will be hard for us to focus on your change.</li> <li>Ensure local tests pass.</li> <li>Commit to your fork using clear commit messages.</li> <li>Send us a pull request, answering any default questions in the pull request interface.</li> <li>Pay attention to any automated CI failures reported in the pull request, and stay involved in the conversation.</li> </ol> <p>GitHub provides additional document on forking a repository and creating a pull request.</p>"},{"location":"contributing.html#finding-contributions-to-work-on","title":"Finding contributions to work on","text":"<p>Looking at the existing issues is a great way to find something to contribute on. As our projects, by default, use the default GitHub issue labels (enhancement/bug/duplicate/help wanted/invalid/question/wontfix), looking at any 'help wanted' issues is a great place to start.</p>"},{"location":"contributing.html#code-of-conduct","title":"Code of Conduct","text":"<p>This project has adopted the Amazon Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opensource-codeofconduct@amazon.com with any additional questions or comments.</p>"},{"location":"contributing.html#security-issue-notifications","title":"Security issue notifications","text":"<p>If you discover a potential security issue in this project we ask that you notify AWS/Amazon Security via our vulnerability reporting page. Please do not create a public github issue.</p>"},{"location":"contributing.html#licensing","title":"Licensing","text":"<p>See the LICENSE file for our project's licensing. We will ask you to confirm the licensing of your contribution.</p>"},{"location":"customizations.html","title":"Customizations","text":""},{"location":"customizations.html#custom-events-queries-sample-walkthroughs","title":"Custom events + queries (sample walkthroughs)","text":"<pre><code>- non-real-time\n- real-time\n</code></pre>"},{"location":"customizations.html#custom-etl","title":"Custom ETL","text":""},{"location":"customizations.html#optional-glue-iceberg-parameter-setup","title":"(Optional) Glue Iceberg Parameter Setup","text":"<p>If the data lake is configured with Apache Iceberg, Glue configuration parameters need to be specified to enable Apache Iceberg for Spark jobs. These can be specified under default parameters</p> <ul> <li> <p>Create a new parameter with the key <code>--datalake-formats</code>. Set the value to be <code>iceberg</code>.</p> </li> <li> <p>Create a new parameter with the key <code>--enable-glue-datacatalog</code>. Set the value to be <code>true</code>.</p> </li> <li> <p>Create a new parameter with the key <code>--conf</code>. Set the value to be <code>spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog --conf spark.sql.catalog.glue_catalog.warehouse=s3://&lt;ANALYTICS_S3_BUCKET_NAME&gt;/ --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO</code>. </p> <ul> <li>Replace <code>&lt;ANALYTICS_S3_BUCKET_NAME&gt;</code> with the name of the created S3 bucket for analytics.</li> </ul> </li> </ul> <p>You can view more on setting up Iceberg with Glue jobs here.</p>"},{"location":"customizations.html#custom-real-time-metrics","title":"Custom Real-Time Metrics","text":"<p>For live analytics, this solution deploys an Amazon Managed Service for Apache Flink application. This application utilizes PyFlink with the Flink Table API to build custom metrics using SQL. Please see the Flink Table API Tutorial to learn more.</p> <p>The pre-created job defines two streams, a source and a sink. These are defined using a CREATE TABLE command with the Amazon Kinesis Data Streams SQL Connector.</p> <p>The source reads from the input Kinesis data stream and the sink writes to the output Kinesis data stream. These streams are configured using variables loaded from the application's configured runtime properties.</p>"},{"location":"customizations.html#source-stream-properties","title":"Source Stream Properties","text":"<ul> <li>Since data is nested in the <code>event</code> JSON attribute of the message, the Flink <code>ROW</code> data type is utilized to define known attributes and make them accessible using a dot notation. <ul> <li>This is done for the attributes <code>event_version</code>, <code>event_id</code>, <code>event_type</code>, <code>event_name</code>, <code>event_timestamp</code>, and <code>event_data</code>.</li> </ul> </li> <li><code>event_data</code> contains a nested JSON object, of which has a user-defined schema that varies depending on the event. Since the schema changes dpeending on the event type, it is extracted as a <code>STRING</code> data type. <ul> <li>To retrieve values nested in the object, the <code>JSON_VALUE</code> function is used within aggregation queries.</li> </ul> </li> <li><code>rowtime</code> is retrieved explicitly from <code>event.event_timestamp</code> object and converted into a <code>TIMESTAMP_LTZ</code> data type attribute. This makes the event time accessible for use in windowing functions. <code>rowtime</code> is used for watermarking within Flink. </li> </ul>"},{"location":"customizations.html#modifying-schema","title":"Modifying schema","text":""},{"location":"customizations.html#modifyingextending-architecture","title":"Modifying/extending architecture","text":"<ul> <li>Allow both Redshift and non-redshift</li> </ul>"},{"location":"customizations.html#modifying-dashboards-ops-and-analytics","title":"Modifying dashboards (ops and analytics)","text":""},{"location":"customizations.html#configuring-access-to-opensearch-ui","title":"Configuring Access to OpenSearch UI","text":"<p>The deployed OpenSearch Application can be configured to allow users access through SAML federation. </p> <p>For this, three components are needed:</p> <ol> <li> <p>Set up SAML Federation to connect your identity provider with the OpenSearch Interface. The federation must be set up to assume a created IAM role. The steps to do so can be found here.</p> </li> <li> <p>Update the IAM role that users will assume when accessing the dashboard. This role must have access to OpenSearch Serverless and OpenSearch UI. Example permissions for configuring OpenSearch Serverless can be found here, permissions to allow access to OpenSearch UI can be found here.</p> </li> <li> <p>Update the data access policy to grant the created IAM role permission to access the OpenSearch Serverless collection. More information on OpenSearch Serverless data access policies can be found here. </p> </li> </ol> <p>After these steps are complete, users can use their logins to access the dashboard and create visualizations.</p>"},{"location":"customizations.html#creating-visualizations-and-dashboards-with-opensearch","title":"Creating Visualizations and Dashboards with OpenSearch","text":"<p>The metric data stored in the OpenSearch index can be used to create visualizations which can be combined into dashboards. OpenSearch offers a variety of visualizations as well as the Dashboards Query Language (DQL) to filter data.</p> <p>A strong visualization for metrics are time-series visualizations. These can be created using the Time-Series Visual Builder (TSVB) visualization tool.</p> <p>Since different metrics have different dimension data, it is strongly recommended to filter data for a specific metric name before proceeding with metric creation. This can be done using the DQL filter under options.     </p>"},{"location":"customizations.html#todo-send-from-game-engines","title":"(TODO: Send from game engines)","text":"<p>Utilize the integrated HTTP libraries in your game engine to form and send requests to the Send Events API.</p> <ul> <li> <p>Unreal Engine 5</p> </li> <li> <p>Unity</p> </li> <li> <p>Godot</p> </li> </ul>"},{"location":"design-considerations.html","title":"Design Considerations","text":"<p>This page explains what drives the team's core decision-making for service selection, feature support, or component design. We use customer feedback to drive our tenets and decision-making, and encourage feedback through GitHub Issues on the guidance repository</p>"},{"location":"design-considerations.html#core-tenets","title":"Core Tenets","text":"<p>The Game Analytics Pipeline Guidance team has the current tenets defined as the guidance's primary goals, and should ultimately allow users to quickly spin up an architecture that:</p> <ol> <li>Allows you to easily start deploying up-to-date analytics best practices for the game industry</li> <li>Has an AWS opinionated balance of cost, performance, least management overhead, and scalability based on aggregate user feedback</li> <li>Can be extensible architecturally for specific user needs</li> </ol> <p> To address how we conclude our opinion on the balance of cost, performance, least management overhead, and scalability, we use feedback we have aggregated from AWS customers and users of the guidance, and continue to iterate and re-aggregate this data. Currently the data concludes the following:</p> <ul> <li>We want to prioritize least management overhead over cost to a certain cutoff (which we will continue to assess and tune)</li> <li>We want default scalability and performance options that address most use cases, but need documented manual scaling options, limits, and controls</li> <li>We want to ensure integration or a path for up-to-date mainstream alternative options</li> </ul>"},{"location":"design-considerations.html#services","title":"Services","text":"<p>Why not use compute fleets (EC2, EKS, ECS, etc) for data processing?</p> <ul> <li>The Game Analytics Pipeline Guidance leans on AWS Managed Services due to their ability to address the above management overhead and default scaling and performance options.</li> </ul> <p>Why not Amazon EMR Serverless?</p> <ul> <li>Glue and Athena provides interfaces for jobs/queries and direct console integration, and EMR requires spark code and notebook just to write scripts. This lets Glue and Athena reduce time to getting started, and management overhead. This does not mean EMR is a worse option, Glue just aligns with the specific above tenets more. The team is open to an EMR deployment option based on user feedback.</li> </ul> <p>Why not Sagemaker Unified Studio, or Lakehouse?</p> <ul> <li>This feature is in Preview as of this document, and once it is fully Globally Available, the team will re-assess.</li> </ul> <p>Why not use Kafka or Amazon Managed Service for Kafka (MSK)?</p> <ul> <li> <p>The team has put substantial effort to integrate Kafka into the guidance, but are limited by the following factors:</p> <ol> <li>Non-managed Kafka has management overhead that conflicts with the guidance tenets</li> <li>Amazon Managed MSK meets all of the architectural needs of the guidance, but limitations with IaC (CDK or Terraform) in which deploying Kafka requires live compute to execute the commands needed to create the initial Kafka topic. This would require a custom resource deployment which greatly complicates the deployment process and diagnosing deployments of the guidance, conflicting with the guidance tenets.</li> </ol> </li> <li> <p>For the time being, the team plans to add documentation to extend the solution for Kafka in the Customizations page, and integrate Kafka support as a direct feature once in-place topic creation is supported for MSK.</p> </li> </ul> <p>Why Glue Workflow instead of Amazon Managed Workflows for Apache Airflow (MWAA)?</p> <ul> <li>MWAA requires a VPC and all associated networking resources to be created, which adds more networking management overhead and more network-related performance considerations that are otherwise all managed under the hood by Glue Workflows. However, Glue Workflows is only constrained to Glue, whereas MWAA supports more options outside of Glue, which is a case of balancing conflicting aspects of the guidance's tenets. Currently we weigh the less management overhead option over the integration for mainstream alternative options. We are open to feedback to re-align MWAA based on user feedback through Github Issues on the repository.</li> </ul> <p>Why use API Gateway?</p> <ul> <li> <p>Compared to direct code to the respective AWS services, or event buses like SQS/EventBridge, API Gateway provides the following benefits:</p> <ul> <li>An authorization workflow using an Authorizer Lambda to ensure events are sending to the correct game/application and not cross-contaminate events. (Up-to-date best practices)</li> <li>A universal endpoint (RESTful HTTP/HTTPS) that is more compatible and less custom code or libraries required (Less Management Overhead)</li> </ul> </li> <li> <p>Compared to a Lambda endpoint, API Gateway also provides direct pass-through that would be cheaper, and rejected API calls are still sent to CloudWatch logs, which would be more simplified and managed than Dead-letter-queue or forwarding logic from Lambda</p> </li> </ul> <p>Why can't I deploy both the Data Lake and Redshift option at the same time?</p> <ul> <li>The choice between Data Lake mode and Redshift mode boils down to your query performance needs and amount of data scanned. The vast majority of customers will only need one option or the other, and in the rare case that there are mixed cases, you can visit the Customizations page to allow both options (Extensibility tenet). Allowing both as a default would create decision paralysis and perceived complexity in the deployed infrastructure, so we skim down the infrastructure to base necessities to address the above core tenets.</li> <li>For infrastructure, the Redshift Data Warehouse option has an integration to store in S3 as a Data Lake store, and has features such as Redshift Spectrum to allow the same query experience regardless of the data store (S3 or Redshift). </li> </ul> <p>Why is there a Kinesis Data Stream in between Flink and Opensearch for Real-Time-Analytics? - The Flink connector for OpenSearch currently only supports authentication via username/password which does not play well with serverless and CDK (encoding password via plaintext, no CDK construct to create it, etc). Kinesis Data Streams allows us to have an intermediary stream for Flink to output to. From there, we can ingest it using a separate OpenSearch ingestion pipeline. The pipeline does not require credential passing and handles things like scaling and failed delivery streams, and leaves the door open for other integrations if needed, such as CloudWatch metrics (V1/V2 style) or firehose.</p>"},{"location":"design-considerations.html#processes","title":"Processes","text":"<p>Do I deploy the Game Analytics Pipeline guidance for each game/application?</p> <ul> <li>The Game Analytics Pipeline Guidance is built to support multiple games, called <code>Applications</code> in the guidance, for cross-game and per-game analytics. This means usually you would only need to deploy a single pipeline per environment.</li> </ul> <p>When should I use Data Lake mode vs Redshift mode?</p> <p>The choice between AWS Data Lake mode and Redshift mode primarily depends on your latency requirements, query complexity, data volume, and concurrency needs.</p> <p>Redshift mode is ideal for scenarios demanding true real-time access to data, complex analytical queries, high-performance requirements, and support for high volume queries and high concurrency. </p> <p>On the other hand, Data Lake mode is more appropriate when near real-time access is sufficient, and is a cost-effective solution for lower data volumes per query (typically less than 10GB) and lower concurrency needs. This mode also excels in scenarios requiring ad-hoc querying capabilities. </p> <p>Kinesis data streams can be enabled with either solution, however, Firehose can only be enabled with a Data Lake for near-real-time analytics. This makes data lake mode more ideal to collect data for batch processing.</p> <p>When should I utilize real-time analytics?</p> <ol> <li> <p>Live events such as initial launches, new version launches, live-stream events, in-game events in which insights are time-sensitive to gather and action on</p> </li> <li> <p>Live-Ops best practices for comparing on-the-ground insights vs aggregate insights. For example, sometimes checking for insights at particular (or random) points in time can provide different insights than viewing data in larger aggregated time windows. Viewing both and comparing can provide good quality insights.</p> </li> <li> <p>Investigating bugs or player reports as resolution in real-time, especially if they are ongoing</p> </li> </ol> <p>Why do we use <code>application_id</code>, and using a DynamoDB table for managing them?</p> <ul> <li>This is to address customers who would like to perform analytics across multiple games, or to ensure analytics within a single game when ingesting from multiple games. The analytics pipeline can query and partition for a single <code>application_id</code>, which are stored and managed from a DynamoDB table alongside authentication secrets to prevent cross-contamination. The DynamoDB table can be interfaced via API endpoint or directly on the AWS Console.</li> </ul> <p>Why is the Schema the way we designed it?</p> <ul> <li> <p>Sample Event Schema for reference: <pre><code>{\n    \"event_id\": \"34c74de5-69d9-4f06-86ac-4b98fef8bca9\",\n    \"event_name\": \"login\",\n    \"event_type\": \"client\",\n    \"event_version\": \"1.0.0\",\n    \"event_timestamp\": 1737658977,\n    \"app_version\": \"1.0.0\",\n    \"event_data\":\n    {\n        \"platform\": \"pc\",\n        \"last_login_time\": 1737658477\n    }\n}\n</code></pre></p> </li> <li> <p>The default Schema for the guidance has a high-level schema structure for events that all events should have uniformly, and a nested <code>event_data</code> structure for event-specific variables. This separation allows schema standardization enforcement while allowing flexibility. As schemas and event types change, the <code>event_version</code> can track the versioning, and <code>app_version</code> can track against game versions. </p> </li> </ul>"},{"location":"getting-started.html","title":"Getting Started","text":"<p>This guide is intended for users integrating game analytics pipeline for the first time. If you have an existing Game Analytics Pipeline deployment and need to upgrade to the latest version, see the Upgrading page.</p>"},{"location":"getting-started.html#prerequisites","title":"Prerequisites","text":"<p>The following resources are required to install, configure, and deploy the game analytics pipeline. </p> <ul> <li>Amazon Web Services Account</li> <li>GitHub Account</li> <li>Visual Studio Code*</li> <li>API Client: Postman Desktop or Bruno</li> <li>IAM Users + Credentials<ul> <li>IAM User for deploying the guidance Infrastructure-as-Code resources</li> <li>IAM User with AWS Console access</li> <li>IAM User for administrating the API, requires credentials (Access Key, Secret Access Key)</li> </ul> </li> </ul> <p>Info</p> <p>When using Access Keys and Secret Access Keys, a best practice is to periodically rotate them. This means your administrators and deployments will need to keep the rotation of your keys in mind as well, more information here</p> <p>*Other code editors can also be used, but tooling support may be limited</p>"},{"location":"getting-started.html#installation","title":"Installation","text":"<ol> <li> <p>Log into your GitHub account, and navigate to the the Game Analytics Pipeline repository</p> </li> <li> <p>Fork into your GitHub account</p> </li> <li> <p>From your fork, clone your repository to a local folder on your machine</p> </li> <li> <p>Navigate to the root of the local folder and open the project in your code editor</p> </li> </ol>"},{"location":"getting-started.html#set-up-environment","title":"Set up Environment","text":"Dev Container (Recommended)Manual Install <p>A development container configuration contains the necessary Python, NodeJS, and the AWS CDK installations and versions needed to implement this guidance, which saves time installing manually. It is recommended, that you use the pre-configured environment as your development environment.</p> <p>To use Dev Containers, a container platform such as Docker Desktop (local) or Finch must be installed and running.</p> <p>Before deploying the sample code, ensure that the following required tools have been installed:</p> <ul> <li>Docker Desktop (local) or Finch</li> <li>Apache Maven</li> <li>AWS Cloud Development Kit (CDK) 2.92 or Terraform</li> <li>Python &gt;=3.8</li> <li>NodeJS &gt;= 22.0.0</li> </ul> <p>If Finch is installed, set the <code>CDK_DOCKER</code> environment variable to <code>finch</code></p> <pre><code>CDK_DOCKER=\"finch\"\n</code></pre> <p>This can also be added to the <code>~/.bashrc</code> file to be configured for every interactive shell. If you are using mac, replace <code>~/.bashrc</code> with <code>~/.zshrc</code> <pre><code>echo 'CDK_DOCKER=\"finch\"' &gt;&gt; ~/.bashrc \n</code></pre></p> <p>Warning</p> <p>The NPM commands to build and deploy the project are written to use UNIX shell commands. Because of this, the manual install is incompatible with the Windows Powershell without modifications to the NPM commands. Please consider using the Dev Container to have a consistent deployment environment.</p>"},{"location":"getting-started.html#install-the-dev-container-extension-for-vscode","title":"Install the Dev Container Extension for VSCode","text":"<ol> <li> <p>Navigate to the Dev Containers extension page in the Visual Studio Marketplace</p> </li> <li> <p>Click Install to add the extension to VSCode</p> </li> </ol> <p>*Other code editors such as the Jetbrains suite also support Dev Containers.</p>"},{"location":"getting-started.html#optional-configure-vscode-to-use-finch","title":"(Optional) Configure VSCode to use Finch","text":"<p>Finch is an open source client for container development. To use Finch, follow the instructions in the Finch documentation to install and initialize Finch for your chosen operating system.</p> <p>After Finch is installed and running, follow the instructions in the Finch documentation to configure the Dev Container Extension to utilize Finch as the container platform to run the dev container for your chosen operating system.</p>"},{"location":"getting-started.html#using-the-dev-container","title":"Using the Dev Container","text":"<p>After following the instructions in Installation, when the project is opened in your code editor, a popup will appear indicating that the folder contains a dev container configuration. To utilize the Dev Container environment, click on \u201cReopen in Container\u201d.</p>"},{"location":"getting-started.html#configuration","title":"Configuration","text":"<p>The Game Analytics Pipeline can be deployed using AWS Cloud Development Kit (CDK) or Terraform. </p> <ol> <li> <p>To select your deployment option, open the <code>package.json</code> file at the root of the repository. </p> </li> <li> <p>At the top of the <code>package.json</code> file there is a <code>\"config\"</code> block. Set the <code>\"iac\"</code> config option to <code>\"cdk\"</code> to use the CDK deployment option or <code>\"tf\"</code> to use the Terraform deployment option.</p> <pre><code>\"config\": {\n    \"iac\": \"cdk\" | \"tf\"\n},\n</code></pre> <p>Warning</p> <p>Avoid changing this option after the stack is deployed without first destroying the created resources. </p> </li> <li> <p>Before deploying the sample code, deployment parameters need to be customized to suite your specific usage requirements. Guidance configuration, and customization, is managed using a <code>config.yaml</code> file, located in the infrastructure folder of the repository.</p> </li> <li> <p>A configuration template file, called <code>config.yaml.TEMPLATE</code> has been provided as a reference for use case customizations. Using the provided devcontainer environment, run the following command to copy the template to  <code>./infrastructure/config.yaml</code>:     <pre><code>cp ./infrastructure/config.yaml.TEMPLATE ./infrastructure/config.yaml\n</code></pre></p> </li> <li> <p>Open the <code>./infrastructure/config.yaml</code> file for editing. Configure the parameters for the pipeline according to the options available in the Config Reference.</p> </li> <li> <p>Terraform Only - Terraform does not use a default region like CDK does, and needs to specify the region in the providers file (<code>./infrastructure/terraform/src/providers.tf</code>). It is defaulted to <code>us-east-1</code> but please modify the below section on the file to your desired region code:</p> <pre><code>provider \"aws\" {\nregion = \"REGION\"\n}\n</code></pre> </li> </ol>"},{"location":"getting-started.html#cdk-only-configuring-esbuild","title":"CDK Only - Configuring ESBuild","text":"<p>This repository utilizes L2 constructs for Nodejs based lambda functions. These constructs handle the bundling of lambda code for deployment. By default, the construct will utilize a Docker container to compile the function, however, this option leads to high build times before each deployment and can have increased performance impacts on MacOS and docker-in-docker enviornments. If <code>esbuild</code> is installed, the L2 construct will build the function using <code>esbuild</code> instead which leads to faster build times.</p> <p>To install <code>esbuild</code>, navigate to the root of the repository and enter the following command:</p> <pre><code>npm install .\n</code></pre> <p><code>esbuild</code> is listed as a development dependency under <code>package.json</code> and will be installed.</p>"},{"location":"getting-started.html#aws-cli-configuration","title":"AWS CLI Configuration","text":"<p>The AWS CLI must be properly configured with credentials to your AWS account before use. The <code>aws configure</code> or <code>aws configure sso</code> commands in your development enviornment terminal are the fastest way to set up your AWS CLI depending on your credential method. Based on the credential method you prefer, the AWS CLI prompts you for the relevant information. </p> <p>More information about the aws configure command can be found in the documentation for the AWS Command Line Interface.</p>"},{"location":"getting-started.html#deployment","title":"Deployment","text":"<p>Info</p> <p>Security credentials for the target AWS account must be configured on the machine before deploying the pipeline. This lets AWS know who you are and what permissions you have to deploy the pipeline. These credentials must have permissions to create new resources within the account, including new IAM Roles.</p> <p>There are different ways in which you can configure programmatic access to AWS resources, depending on the environment and the AWS access available to you. Please consult the following documentation based on your deployment option to configure the credentials before proceeding with this section.</p> <ul> <li> <p>AWS Cloud Development Kit (CDK)</p> </li> <li> <p>HashiCorp Terraform</p> </li> </ul> <p>Once you have set your own custom configuration settings, and saved the config.yaml file, then following steps can be used to deploy the game analytics pipeline:</p> <ol> <li>Build the sample code dependencies, by running the following command: <pre><code>npm run build\n</code></pre></li> <li>Bootstrap the sample code by running the following command: <pre><code>npm run deploy.bootstrap\n</code></pre></li> <li>Deploy the sample code, by running the following command: <pre><code>npm run deploy\n</code></pre> After deployment is complete, a list of outputs will be posted to the terminal. These are names and references to relevant deployed assets from the stack. Please note these for future reference.</li> </ol>"},{"location":"getting-started.html#start-initial-application-and-api","title":"Start initial Application and API","text":"<p>Before sending events to the pipeline, an Application and corresponding Authorization key will need to be created. A Postman collection file is provided to help configure Postman or Bruno for use with the solution. </p> <ol> <li>Locate the API Endpoint from the output after deployment. Note this down for the collection<ul> <li>If deployed using CDK, this endpoint is the value of <code>CentralizedGameAnalytics.ApiEndpoint</code>. </li> <li>If deployed using Terraform, this endpoint is the value of <code>api_endpoint</code></li> </ul> </li> <li>The collection file is located at <code>/resources/game-analytics-pipeline-postman-collection.json</code></li> </ol> PostmanBruno <ol> <li>For instructions on how to import a collection, refer to the documentation for your selected API Client: Import Postman data</li> <li>Once the collection is imported into Postman, create a new environment by selecting Environments in the sidebar and select the Add icon. You can also select the environment selector at the top right of the workbench and select Add icon. Enter a name for your new environment.</li> <li>In order to perform administrator actions on your API, Authentication must be configured to utilize SigV4 authentication for an IAM identity. These credentials inherit from your <code>access_key</code> and <code>secret_access_key</code> variables configured in the collection. For more information, refer to Authenticate with AWS Signature authentication workflow in Postman</li> <li>Replicate the following image for your environment (Note: leave <code>application_id</code> blank. This will be filled in later):<ul> <li>The AWS Access Key and Secret Access Key is specifically for administrating the API only, and should not be used by event sources. You should have an IAM User specifically with these credentials with sufficient permissions to run the tasks on the API, but for security best practice to use least privilege, a sample policy is created by the guidance <code>{WORKLOAD_NAME}-AdminAPIAccess</code> to attach to the user to perform only the admin tasks </li> </ul> </li> <li>Ensure there are no trailing return/enter spaces at the end of the variables, and click \"Save\" on the top right.</li> <li>Select your newly created and saved environment by navigating to the top right drop-down menu that says <code>No environment</code>, selecting on it and selecting your new environment</li> </ol> <ol> <li>For instructions on how to import a collection, refer to the documentation for your selected API Client: Importing Enviornment into Bruno</li> <li>Once the collection is imported into your API client, navigate to the Vars tab for the collection. <ul> <li>Validate that five variables (<code>api_base_path</code>, <code>application_id</code>, <code>access_key</code>, <code>secret_access_key</code>, and <code>region</code>) are under Pre Request variables. If they are not, create variables with those names.</li> <li>Configure the collection-wide <code>api_base_path</code> variable to be your deployed API base path. The value of the path should be the URL retrieved from step 1.</li> <li>Configure <code>region</code> to be the region where the stack is deployed</li> <li>Configure <code>access_key</code> to be the access key of the identity used to deploy the stack</li> <li>Configure <code>secret_access_key</code> to be the secret access key of the identity used to deploy the stack</li> <li>Leave <code>application_id</code> blank. This will be filled in later.</li> </ul> </li> <li> <p>Ensure the collection variables are created</p> <p></p> </li> <li> <p>In order to perform administrator actions on your API, Authentication must be configured to utilize SigV4 authentication for an IAM identity. These credentials inherit from your <code>access_key</code> and <code>secret_access_key</code> variables configured in the collection. For more information, refer to  Authenticate using AWS Signature</p> <ul> <li>If a session token is needed for temporary credentials, please add them manually</li> </ul> </li> <li>Ensure there are no trailing return/enter spaces at the end of the variables. Save the configuration by pressing <code>ctrl + s</code> (or <code>cmd + s</code> on mac).</li> </ol> <p>After the pipeline is deployed, a new application must be created using the Application API. </p>"},{"location":"getting-started.html#create-a-new-application","title":"Create a new Application","text":"<p>The Game Analytics Pipeline Guidance is built to support multiple games, called <code>Applications</code> in the guidance, for cross-game and per-game analytics. This means usually you would only need to deploy a single pipeline per environment.</p> <ul> <li>Navigate under the Applications tab of the collection and select the Create Application API. </li> <li>Navigate to the <code>Body</code> tab (under the address bar showing the API path) and modify the value of Name and Description in the json to match your game.</li> <li>Send the API request. Note the value of the <code>\"ApplicationId\"</code> in the API response.</li> <li>Copy the value of the ApplicationId and paste it in to the <code>application_id</code> value for the collection. This will allow the rest of your API calls to interact with the application</li> </ul> <p>Note</p> <p>Refer to the API Reference for POST - Create Application for more information on how to register a new application. </p> <p>After the application is created, create an API key to send events to the API. Make sure to go back to Postman or Bruno if you are using them to update the environment variables (and save).</p>"},{"location":"getting-started.html#create-a-new-api-key","title":"Create a new API Key","text":"<ul> <li>Navigate under the Authorizations tab of the collection and select the Create Authorization API.</li> <li>The <code>\"ApplicationId\"</code> from the previous step should be passed in the API path automatically. </li> <li>Navigate to the <code>Body</code> tab (under the address bar showing the API path) and modify the value of Name and Description in the json to match your game.</li> <li>Send the API request. Note the value of the <code>\"ApiKeyValue\"</code> in the API response.</li> <li>Refer to the API Reference for POST - Create API Key for Application for more information on how to create a new authorization key. </li> </ul> <p>If you have Redshift Mode enabled, enable the materialized views and remaining infrastructure through the API. Refer to the API Reference for POST - Setup Redshift on how to setup the final Redshift components.</p>"},{"location":"getting-started.html#apache-iceberg-only-configure-table-partition-spec","title":"Apache Iceberg Only - Configure Table Partition Spec","text":"<p>If the <code>ENABLE_APACHE_ICEBERG_SUPPORT</code> configuration is set to <code>true</code>, a basic Apache Iceberg table is created in the glue catalog with the table name specified with <code>RAW_EVENTS_TABLE</code> under the database specified with <code>EVENTS_DATABASE</code>. </p> <p>By default, this table does not contain a configured partition specification. To enable partitioning, a Glue job must be run before data is ingested to configure the table.</p> <ol> <li>Locate the Iceberg Setup Job Name from the deployment outputs. Note this down for later.<ul> <li>The name of the job is the value of <code>CentralizedGameAnalytics.IcebergSetupJobName</code> when using CDK.</li> <li>The name of the job is the value of <code>iceberg_setup_job_name</code> when using Terraform.</li> </ul> </li> <li>Navigate to the Glue AWS Console. Ensure that you are in the same region that the stack is deployed in</li> <li>On the left sidebar, navigate to ETL jobs</li> <li>Locate the deployed setup job with the name retrieved from Step 1 in the list of jobs. Use the search bar if necessary</li> <li>Click on the checkbox to the left of the name of the job. </li> <li>Click on Run job at the top right of the job list to start the job. </li> <li>Navigate to the job run status using the popup at the top of the page. Monitor the status until the job is complete and successful.</li> </ol>"},{"location":"getting-started.html#real-time-only-starting-flink","title":"Real Time Only - Starting Flink","text":"<p>If the <code>REAL_TIME_ANALYTICS</code> configuration is set to <code>true</code>, a Flink Application will be created. This application needs to be in the <code>RUNNING</code> state for incoming events to be processed in real time. </p> <ol> <li> <p>Locate the Flink App Name from the deployment outputs. Note this down for later.</p> <ul> <li>The name of the job is the value of <code>CentralizedGameAnalytics.FlinkAppName</code> when using CDK.</li> <li>The name of the job is the value of <code>flink_app_name</code> when using Terraform.</li> </ul> </li> <li> <p>Navigate to the AWS Console. Open the console for Managed Apache Flink</p> </li> <li> <p>Click on the Apache Flink applications page on the side menu. </p> </li> <li> <p>Navigate to the application with the name matching the one retrieved from the step 1 output.</p> </li> <li> <p>Click on the Run button at the top right of the menu. Configure the Snapshots option to Run without snapshot when starting for the first time. Click on the Run button again to start the application.</p> </li> <li> <p>Wait for the Status to show as Running</p> </li> </ol> <p>For more information and troubleshooting, refer to the documentation for Run a Managed Service for Apache Flink application.</p>"},{"location":"getting-started.html#send-events-to-the-pipeline","title":"Send Events to the Pipeline","text":"<p>This project contains a python script to send a simulated stream of events to the pipeline. The script is located at <code>resources/publish-data/handler.py</code>. To run the script, run the following <pre><code>python resources/publish-data/handler.py --api-path &lt;API_PATH&gt; --api-key &lt;API_KEY&gt; --application-id &lt;APPLICATION_ID&gt;\n</code></pre></p> <ul> <li>Replace <code>&lt;API_PATH&gt;</code> with the value of the API endpoint retrieved after deployment during step 1 of Start initial Application and API</li> <li>Replace <code>&lt;APPLICATION_ID&gt;</code> with the value of <code>\"ApplicationId\"</code> created during the Create a new Application step.</li> <li>Replace <code>&lt;API_KEY&gt;</code> with the value of <code>\"ApiKeyValue\"</code> created during the Create a new API Key step.</li> </ul> <p>For more information, refer to the API Reference for POST - Send Events for the REST API schema to send events to the pipeline.</p>"},{"location":"getting-started.html#verify-and-query-event-data","title":"Verify and Query Event Data","text":"Data Lake ModeRedshift ModeReal-Time Analytics <ol> <li> <p>Navigate to the AWS Console for Athena</p> </li> <li> <p>If you are not already on the query editor, click on the Launch query editor button</p> </li> <li> <p>At the top left of the editor, select the workgroup for your stack. The name should consist of the name specified by <code>WORKLOAD_NAME</code> in config.yaml followed by the suffix <code>-workgroup</code> and a random suffix.</p> <p></p> </li> <li> <p>Acknowledge the settings for the workgroup</p> </li> <li> <p>On the left hand side, select the Database with the name specified by <code>EVENTS_DATABASE</code> in config.yaml</p> <p></p> </li> <li> <p>A list of tables should appear below the selection. Select a table, click the three buttons on the left, and select At to see the items in the table.</p> <p></p> </li> <li> <p>To use the pre-defined queries, select Saved queries at the top toolbar of the query editor. This will show a list of queries created for the stack. </p> <p></p> </li> <li> <p>To run a query, click on the highlighted ID of the query to open it in a new query editor tab and then press Run for the query. View the results below after the query finishes executing.</p> <p></p> </li> </ol> <ol> <li> <p>In the AWS Console, navigate to Redshift and you will see the Serverless Dashboard.</p> <p></p> </li> <li> <p>Select your namespace and then press the Query Editor button in the top right.</p> <p></p> </li> <li> <p>In the Query Editor, you will see your namespace in the top left.</p> <p></p> </li> <li> <p>Select the ... to the right of the name to create a connection. Choose Secrets Manager and the relevant secret.</p> <p></p> </li> <li> <p>Navigate to <code>native databases / events / Views</code></p> <p></p> </li> <li> <p>Double click one of the views to automatically open a query in the editor on the right side. Edit the query and press Run when ready.</p> </li> <li> <p>The view event_data is the materialized view which reads directly from the Kinesis Data Stream. The other views are essentially pre-made queries against the event_data materialized view.</p> </li> </ol> <p>If <code>REAL_TIME_ANALYTICS</code> is set to <code>true</code>, an OpenSearch Serverless collection will be created to store and index the time series metrics emitted by the Managed Service for Apache Flink application that is initiated in the Starting Flink step. </p> <p>An acccompanying OpenSearch UI Application is created to query and visualize the data emitted by real time analytics. To access this application, ensure you are logged in to the AWS console in your browser of choice with the created OpenSearch Admin IAM role.</p> <ol> <li>Ensure you are logged in to the AWS console with an existing administrator IAM role. This account must have the <code>sts:AssumeRole</code> permission to switch roles.</li> <li>Locate the OpenSearch Admin Assume Link to assume the IAM role from the output of the deployed stack. <ul> <li>If deployed using CDK, this output is identified by <code>CentralizedGameAnalytics.OpenSearchAdminAssumeLink</code>. </li> <li>If deployed using Terraform, this output is identified by <code>opensearch_admin_assume_link</code></li> </ul> </li> <li>Paste the link into your browser of choice, confirm the details are correct, and click Switch Role to assume the admin role. The assumed role should have the name of your configured <code>PROJECT_NAME</code> followed by <code>-OpenSearchAdmin</code>.     </li> <li>Verify that you have assumed the admin IAM role by checking the role at the top right of the AWS console.</li> <li>Locate the OpenSearch Dashboard Link to the OpenSearch application from the output of the deployed stack. <ul> <li>If deployed using CDK, this output is identified by <code>CentralizedGameAnalytics.OpenSearchDashboardLink</code>. </li> <li>If deployed using Terraform, this output is identified by <code>opensearch_dashboard_link</code></li> </ul> </li> <li>Paste the link into your browser of choice. Ensure that you are logged in to the AWS console as the OpenSearch Admin before proceeding. </li> <li>On the main dashboard page, verify that you are logged in as the OpenSearch Admin by clicking on the \"a\" icon on the bottom right and viewing the associated role.     </li> <li>On the main dashboard page, click on Create workspace under Essentials     </li> <li>On the next page, provide a name and description to the workspace</li> <li>Under Associate data sources, click Associate OpenSearch data sources </li> <li>Select the Game Analytics Pipeline Metric Collection and click Associate data sources </li> <li>If needed, change the visibility of the workspace in the last section</li> <li>Click Create workspace</li> <li>On the left side, select Index patterns and click Create index pattern </li> <li>Select the associated data source and click Next</li> <li>In the field for Index pattern name, enter <code>game_metrics*</code> </li> <li>Create the index pattern and verify the fields of the index     </li> <li>View the raw data stored in the index by navigating to the Discover tab on the left. If there is no data shown, adjust the time window using the control at the top right.      </li> </ol> <p>Using the created game_metrics index pattern you can create time series visualizations of the real-time metrics.</p>"},{"location":"getting-started.html#next-steps","title":"Next Steps","text":"<ul> <li>Customizations</li> <li>Troubleshooting</li> <li>References</li> </ul>"},{"location":"roadmap.html","title":"Roadmap","text":""},{"location":"security.html","title":"Security","text":""},{"location":"security.html#overview","title":"Overview","text":"<p>This project is maintained by members of the AWS for Games technical community within AWS (i.e. Solutions Architects, Technical Account Managers, Software Engineers) who support the gaming industry. Design decisions and tradeoffs made throughout this project reflect our experiences working with game studios to build and maintain their development infrastructure and tools in the cloud. We encourage contributions from the community via Pull Requests, which are manually reviewed and tested by the core maintainers of the project before they are merged.</p>"},{"location":"security.html#reporting-a-vulnerability","title":"Reporting a vulnerability","text":"<p>If you discover a potential security issue in this project, we ask that you notify AWS/Amazon Security via our vulnerability reporting page or directly via email to aws-security@amazon.com.</p> <p>Please do not create a public GitHub issue.</p>"},{"location":"troubleshooting.html","title":"Troubleshooting","text":""},{"location":"troubleshooting.html#general-troubleshooting","title":"General troubleshooting","text":""},{"location":"troubleshooting.html#deployment-fails-with-insufficient-permissions-for-lake-formation","title":"Deployment fails with insufficient permissions for Lake Formation","text":"<p>Issue: This occurs when AWS Lake Formation is enabled on the account. AWS Lake Formation is an access control service to centralize fine-grained access control for data lakes. If this is enabled, the IAM role used to deploy the solution needs to have permissions to create and modify resources.</p> <p>Solutions: * If you are using CDK to deploy the stack, after running <code>npm run deploy.bootstrap</code> navigate to CloudFormation in the AWS console. Locate the CDKToolkit stack, navigate to the Resources tab of the stack, and locate the resource <code>CloudFormationExecutionRole</code>. Grant this IAM role admin priviledges in your Lake Formation console. * If you are using Hashicorp Terraform to deploy the stack, ensure that the IAM identity configured has admin priviledges in your Lake Formation console.</p>"},{"location":"troubleshooting.html#update-metadata-in-the-catalogue-after-you-add-compatible-partitions-or-new-data","title":"Update metadata in the catalogue after you add Compatible partitions or new data","text":"<p>Issue: Data is not showed on Amazon Athena.</p> <p>Solutions: * Create an AWS Glue crawler on your data Amazon S3 bucket <code>raw data folder</code>.  * Run the a <code>MSCK REPAIR TABLE</code> command on Amazon Athena to update partitions. Read more here</p>"},{"location":"troubleshooting.html#migration-troubleshooting","title":"Migration troubleshooting","text":""},{"location":"troubleshooting.html#aws-glue-version-compatibility","title":"AWS Glue version compatibility","text":"<p>Issue: Glue jobs failing due to version incompatibility. Solution:</p> <ul> <li>Ensure using Glue version 3.0 or later</li> <li>Add necessary Iceberg libraries to job configuration</li> <li>Update job parameters to include Iceberg catalog settings</li> </ul>"},{"location":"references/api-reference.html","title":"API Reference","text":"<p>The Game Analytics Pipeline API is the entry point for applications to send data, and it provides functionality for administrators to programmatically configure registered applications. The solution supports HTTPS only, using a certificate managed by AWS. For information about configuring a custom domain for your REST API, refer to Setting up custom domain names for REST APIs in the Amazon API Gateway Developer Guide.</p> <p>Info</p> <ul> <li>The base path to the API is: <code>https://{YOUR_API_URL}/live</code> unless you set a different API stage name under the config file.<ul> <li>For example, the full path to one of the API calls could be <code>https://{YOUR_API_URL}/live/applications/{APPLICATION_ID}/authorizations/{API_KEY_ID}</code>.</li> <li>Refer to the Getting Started section to get your API URL, API stage name, and for steps on interacting with the API. Refer to the Component Deep Dive section to understand how the API's authentication and underlying processes work.</li> </ul> </li> <li>Quick link to sending events. Sending events requires an application and it's corresponding authorization token, for explanations see the other API calls first. All other API calls are administrative.</li> </ul>"},{"location":"references/api-reference.html#applications","title":"Applications","text":"<p><code>/applications</code></p>"},{"location":"references/api-reference.html#post-create-application","title":"POST - Create Application","text":"<ul> <li> <p>Description</p> <ul> <li>This operation enables you to register a new application with the solution. Applications represent a specific game/application to perform per-application analytics on.</li> </ul> </li> <li> <p>Request</p> <ul> <li>Body<ul> <li>Name (String) [Required] - Name of the application to register.</li> <li>Description (String) [Optional] - Description of the application. <pre><code>POST\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications\n{\n    \"Name\": \"TestGame\"\n    \"Description\": \"This is a test game\"\n}\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Application information is stored in a DynamoDB Table deployed by the solution. See Component Deep Dive for details on the process. <pre><code>{\n    \"ApplicationId\": \"d76d064f-ca8b-41ff-839f-4735e9a4b69d\",\n    \"ApplicationName\": \"TestGame\",\n    \"Description\": \"This is a test game\",\n    \"UpdatedAt\": \"2025-01-26T21:45:50Z\",\n    \"CreatedAt\": \"2025-01-26T21:45:50Z\"\n}\n</code></pre></p> <ul> <li>ApplicationId (String) - A unique UUID representing the Application. Keep this value for performing actions on this specific Application. See the below <code>GET - List Applications</code> documentation to view all Applications.</li> <li>ApplicationName (String) - The name of the created Application, same as the one sent in the request.</li> <li>Description (String) - The description of the created Application, same as the one sent in the request.</li> <li>UpdatedAt (DateTime) - The date and time the application was last updated.</li> <li>ErrorCode (DateTime) - The date and time the application was created.</li> </ul> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/api-reference.html#get-list-applications","title":"GET - List Applications","text":"<ul> <li> <p>Description</p> <ul> <li>This operation enables you to list the applications that are registered with the solution. Applications represent a specific game/application to perform per-application analytics on.</li> </ul> </li> <li> <p>Request <pre><code>GET\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications\n</code></pre></p> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Application information is stored in a DynamoDB Table deployed by the solution. See Component Deep Dive for details on the process. <pre><code>{\n    \"Applications\": [\n        {\n            \"ApplicationId\": \"d76d064f-ca8b-41ff-839f-4735e9a4b69d\",\n            \"ApplicationName\": \"TestGame\",\n            \"Description\": \"This is a test game\",\n            \"UpdatedAt\": \"2025-01-26T21:45:50Z\",\n            \"CreatedAt\": \"2025-01-26T21:45:50Z\"\n        }\n    ],\n    \"Count\": 1\n}\n</code></pre></p> <ul> <li>Count (Number) - The number of registered applications.</li> <li>Applications (Array) - Array of <code>Application</code> objects representing details on each registered Application:<ul> <li>ApplicationId (String) - A unique UUID representing the Application. Keep this value for performing actions on this specific Application. See the below <code>GET - List Applications</code> documentation to view all Applications.</li> <li>ApplicationName (String) - The name of the created Application, same as the one sent in the request.</li> <li>Description (String) - The description of the created Application, same as the one sent in the request.</li> <li>UpdatedAt (DateTime) - The date and time the application was last updated.</li> <li>ErrorCode (DateTime) - The date and time the application was created.</li> </ul> </li> </ul> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/api-reference.html#per-application","title":"Per-Application","text":"<p><code>/applications/{APPLICATION_ID}</code></p>"},{"location":"references/api-reference.html#get-get-an-applications-detail","title":"GET - Get an Application's Detail","text":"<ul> <li> <p>Description</p> <ul> <li>This operation enables you to describe the details of a registered application. Applications represent a specific game/application to perform per-application analytics on.</li> </ul> </li> <li> <p>Request <pre><code>GET\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}\n</code></pre></p> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Application information is stored in a DynamoDB Table deployed by the solution. See Component Deep Dive for details on the process. <pre><code>{\n    \"ApplicationId\": \"d76d064f-ca8b-41ff-839f-4735e9a4b69d\",\n    \"ApplicationName\": \"TestGame\",\n    \"Description\": \"This is a test game\",\n    \"UpdatedAt\": \"2025-01-26T21:45:50Z\",\n    \"CreatedAt\": \"2025-01-26T21:45:50Z\"\n}\n</code></pre></p> <ul> <li>ApplicationId (String) - A unique UUID representing the Application. Keep this value for performing actions on this specific Application. See the below <code>GET - List Applications</code> documentation to view all Applications.</li> <li>ApplicationName (String) - The name of the created Application, same as the one sent in the request.</li> <li>Description (String) - The description of the created Application, same as the one sent in the request.</li> <li>UpdatedAt (DateTime) - The date and time the application was last updated.</li> <li>ErrorCode (DateTime) - The date and time the application was created.</li> </ul> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/api-reference.html#delete-delete-application","title":"DELETE - Delete Application","text":"<ul> <li>Description<ul> <li>This operation enables you to delete a registered application. Applications represent a specific game/application to perform per-application analytics on.</li> </ul> </li> </ul> <p>Warning</p> <p>Data that was ingested by deleted applications remains in Amazon Simple Storage Service (Amazon S3) after deletion, but new data cannot be submitted to the solution API after an application is deleted. When an application is deleted, all associated API key authorizations are also deleted.</p> <ul> <li> <p>Request <pre><code>DELETE\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}\n</code></pre></p> </li> <li> <p>Response</p> <ul> <li><code>200</code> <pre><code>\"Delete Successful\"\n</code></pre></li> <li><code>4XX/5XX</code> - See the Troubleshooting section for errors.</li> </ul> </li> </ul>"},{"location":"references/api-reference.html#sending-events","title":"Sending Events","text":"<p><code>/applications/{APPLICATION_ID}/events</code></p>"},{"location":"references/api-reference.html#post-send-events","title":"POST - Send Events","text":"<ul> <li> <p>Description</p> <ul> <li>This operation enables you to send a batch game events in a single API request to the Game Analytics Pipeline solution. Please review the Component Deep Dive section for batching/size/service limits.</li> </ul> </li> <li> <p>Request</p> <ul> <li>Header<ul> <li>Authorization (String) [Required] - The API Key's Value/code for the application. See either Getting Started or the below Authorizations/API Key references for high level steps / details on creating or obtaining an API Key. See Component Deep Dive for more details on the process and Design Considerations for reasoning.</li> </ul> </li> <li>Body<ul> <li>Events (Array) [Required] - Array/List of game event JSON objects to send to the pipeline **INSERT NOTE ABOUT SCRIPT/SAMPLE OF GAME EVENTS HERE. <pre><code>POST\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}/events\nAuthorization: \"KKNL09jc1Ub7WQzmZZ+9BNfxLCOhhJGKGkpHyWy+uk6J6WrIj3x8tbJLkIkZUSxzBgT4RyUOOy7ZBKSaj0y2Zg==\"\n{\n    \"events\": [Array/List of Game Event Objects]\n}\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Sent even if there are some unsuccessful events. This includes successful and failed records. <pre><code>{\n    \"Total\": 2,\n    \"FailedRecordCount\": 1,\n    \"Events\": [\n        {\n            \"Result\": \"Ok\",\n            {\n                \"Result\": \"Error\",\n                \"ErrorCode\": \"InvalidAction\"\n            }\n        }\n    ]\n}\n</code></pre></p> <ul> <li>Total (Number) - Number of received events</li> <li>FailedRecordCount (Integer) - Number of failed events in the batch</li> <li>Events (List) - List of each event's result</li> <li>Result (String) - Response message for an event</li> <li>ErrorCode (String) - Response code for an error event. </li> </ul> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/api-reference.html#authorizations","title":"Authorizations","text":"<p><code>/applications/{APPLICATION_ID}/authorizations</code></p>"},{"location":"references/api-reference.html#post-create-api-key-for-application","title":"POST - Create API Key for Application","text":"<ul> <li> <p>Description</p> <ul> <li>This operation generates a new API key that is authorized to send events to a specific Application. When sending events to an Application with the above <code>Sending Events</code> API call, the API Key's value/code is included in the <code>Authorization</code> header for security. See Component Deep Dive for more details on the process and Design Considerations for reasoning.</li> </ul> </li> <li> <p>Request</p> <ul> <li>Body<ul> <li>Name (String) [Required] - Name of the API key to create.</li> <li>Description (String) [Optional] - Description of the API Key being created. <pre><code>POST\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}/authorizations\n{\n    \"Name\": \"TestKey\"\n    \"Description\": \"This is a test key for my game\"\n}\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Key/authorization information is stored in a DynamoDB Table deployed by the solution. See Component Deep Dive for details on the process. <pre><code>{\n    \"ApiKeyId\": \"01af2cb3-8b1f-4bc0-801a-884a30fcb8cd\",\n    \"ApiKeyValue\": \"KKNL09jc1Ub7WQzmZZ+9BNfxLCOhhJGKGkpHyWy+uk6J6WrIj3x8tbJLkIkZUSxzBgT4RyUOOy7ZBKSaj0y2Zg==\",\n    \"ApiKeyName\": \"TestKey\",\n    \"ApplicationId\": \"d76d064f-ca8b-41ff-839f-4735e9a4b69d\",\n    \"ApiKeyDescription\": \"This is a test key for my game\",\n    \"CreatedAt\": \"2025-01-26T21:46:25Z\",\n    \"UpdatedAt\": \"2025-01-26T21:46:25Z\",\n    \"Enabled\": true\n}\n</code></pre></p> <ul> <li>ApiKeyId (String) - A unique UUID representing the key being created.</li> <li>ApiKeyValue (String) - The value of the key. This value is used for the <code>Authorization</code> header when sending events to an Application with the above <code>Sending Events</code> API call</li> <li>ApiKeyName (String) - The name of the created key, same as the one sent in the request.</li> <li>ApplicationId (String) - A unique UUID representing the Application that this key is made for.</li> <li>ApiKeyDescription (String) - The description of the created key, same as the one sent in the request.</li> <li>CreatedAt (DateTime) - The date and time the key was created.</li> <li>UpdatedAt (DateTime) - The date and time the key was last updated.</li> <li>Enabled (Boolean) - Whether the key is enabled or disabled.</li> </ul> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/api-reference.html#get-list-authorizations-for-an-application","title":"GET - List Authorizations for an Application","text":"<ul> <li> <p>Description</p> <ul> <li>This operation enables you to list the API key authorizations associated with an application.</li> </ul> </li> <li> <p>Request <pre><code>GET\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}/authorizations\n</code></pre></p> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Key/authorization information is stored in a DynamoDB Table deployed by the solution. See Component Deep Dive for details on the process. <pre><code>{\n    \"Authorizations\": [\n        {\n            \"ApiKeyId\": \"01af2cb3-8b1f-4bc0-801a-884a30fcb8cd\",\n            \"ApiKeyValue\": \"KKNL09jc1Ub7WQzmZZ+9BNfxLCOhhJGKGkpHyWy+uk6J6WrIj3x8tbJLkIkZUSxzBgT4RyUOOy7ZBKSaj0y2Zg==\",\n            \"ApiKeyName\": \"TestKey\",\n            \"ApplicationId\": \"d76d064f-ca8b-41ff-839f-4735e9a4b69d\",\n            \"ApiKeyDescription\": \"This is a test key for my game\",\n            \"CreatedAt\": \"2025-01-26T21:46:25Z\",\n            \"UpdatedAt\": \"2025-01-26T21:46:25Z\",\n            \"Enabled\": true\n        }\n    ],\n    \"Count\": 1\n}\n</code></pre></p> <ul> <li>Count (Number) - The number of registered authorizations.</li> <li>Authorizations (Array) - Array of <code>Authorization</code> objects representing details on each registered Authorization for the Application:<ul> <li>ApiKeyId (String) - A unique UUID representing the key being created.</li> <li>ApiKeyValue (String) - The value of the key. This value is used for the <code>Authorization</code> header when sending events to an Application with the above <code>Sending Events</code> API call</li> <li>ApiKeyName (String) - The name of the created key, same as the one sent in the request.</li> <li>ApplicationId (String) - A unique UUID representing the Application that this key is made for.</li> <li>ApiKeyDescription (String) - The description of the created key, same as the one sent in the request.</li> <li>CreatedAt (DateTime) - The date and time the key was created.</li> <li>UpdatedAt (DateTime) - The date and time the key was last updated.</li> <li>Enabled (Boolean) - Whether the key is enabled or disabled.</li> </ul> </li> </ul> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/api-reference.html#api-keys","title":"API Keys","text":"<p><code>/applications/{APPLICATION_ID}/authorizations/{API_KEY_ID}</code></p>"},{"location":"references/api-reference.html#get-get-an-authorizations-details","title":"GET - Get an Authorization's Details","text":"<ul> <li> <p>Description</p> <ul> <li>This operation enables you to describe the details of an application's API Key Authorization.</li> </ul> </li> <li> <p>Request <pre><code>GET\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}/authorizations/{API_KEY_ID}\n</code></pre></p> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Key/authorization information is stored in a DynamoDB Table deployed by the solution. See Component Deep Dive for details on the process. <pre><code>{\n    \"ApiKeyId\": \"01af2cb3-8b1f-4bc0-801a-884a30fcb8cd\",\n    \"ApiKeyValue\": \"KKNL09jc1Ub7WQzmZZ+9BNfxLCOhhJGKGkpHyWy+uk6J6WrIj3x8tbJLkIkZUSxzBgT4RyUOOy7ZBKSaj0y2Zg==\",\n    \"ApiKeyName\": \"TestKey\",\n    \"ApplicationId\": \"d76d064f-ca8b-41ff-839f-4735e9a4b69d\",\n    \"ApiKeyDescription\": \"This is a test key for my game\",\n    \"CreatedAt\": \"2025-01-26T21:46:25Z\",\n    \"UpdatedAt\": \"2025-01-26T21:46:25Z\",\n    \"Enabled\": true\n}\n</code></pre></p> <ul> <li>ApiKeyId (String) - A unique UUID representing the key being created.</li> <li>ApiKeyValue (String) - The value of the key. This value is used for the <code>Authorization</code> header when sending events to an Application with the above <code>Sending Events</code> API call</li> <li>ApiKeyName (String) - The name of the created key, same as the one sent in the request.</li> <li>ApplicationId (String) - A unique UUID representing the Application that this key is made for.</li> <li>ApiKeyDescription (String) - The description of the created key, same as the one sent in the request.</li> <li>CreatedAt (DateTime) - The date and time the key was created.</li> <li>UpdatedAt (DateTime) - The date and time the key was last updated.</li> <li>Enabled (Boolean) - Whether the key is enabled or disabled.</li> </ul> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/api-reference.html#delete-delete-an-authorization","title":"DELETE - Delete an Authorization","text":"<ul> <li> <p>Description</p> <ul> <li>This operation enables you to delete an API key associated with an application.</li> </ul> </li> <li> <p>Request <pre><code>DELETE\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}/authorizations/{API_KEY_ID}\n</code></pre></p> </li> <li> <p>Response</p> <ul> <li><code>200</code> <pre><code>\"Delete Successful\"\n</code></pre></li> <li><code>4XX/5XX</code> - See the Troubleshooting section for errors.</li> </ul> </li> </ul>"},{"location":"references/api-reference.html#put-enabledisable-authorization","title":"PUT - Enable/Disable Authorization","text":"<ul> <li>Description<ul> <li>This operation enables you to enable or disable an API key without deleting it from the database.</li> </ul> </li> </ul> <p>Warning</p> <p>API Gateway authorization caching is enabled in the solution API. It may take up to 300 seconds (5 minutes) before a change to the Enabled status of an API key is detected by the LambdaAuthorizer Lambda function. To reduce this time, you can modify or disable the Authorization Cache. Reducing or removing this cache TTL (time-to-live) results in additional queries to the Authorizations DynamoDB table and increases costs.</p> <ul> <li> <p>Request</p> <ul> <li>Body<ul> <li>Enabled (Boolean) [Required] - Enabling or Disabling the key. <pre><code>PUT\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/{APPLICATION_ID}/authorizations/{API_KEY_ID}\n{\n    \"Enabled\": false\n}\n</code></pre></li> </ul> </li> </ul> </li> <li> <p>Response</p> <ul> <li><code>200</code> <pre><code>{\n    \"Enabled\": false\n}\n</code></pre></li> <li><code>4XX/5XX</code> - See the Troubleshooting section for errors.</li> </ul> </li> </ul>"},{"location":"references/api-reference.html#redshift","title":"Redshift","text":"<p><code>/redshift/setup</code></p>"},{"location":"references/api-reference.html#post-set-up-redshift","title":"POST - Set up Redshift","text":"<ul> <li> <p>Description</p> <ul> <li>This operation sets up Redshift materialized views from Kinesis during setup process</li> </ul> </li> <li> <p>Request <pre><code>POST\nhttps://{YOUR_API_ENDPOINT_URL}/live/applications/redshift/setup\n</code></pre></p> </li> <li> <p>Response</p> <ul> <li> <p><code>200</code> - Completes process and returns OK. Can be called multiple times without issues. See Getting Started for details on the process. <pre><code>{\n}\n</code></pre></p> </li> <li> <p><code>4XX/5XX</code> - See the Troubleshooting section for errors.</p> </li> </ul> </li> </ul>"},{"location":"references/config-reference.html","title":"Configuration Reference","text":"<p>The following settings can be adjusted in <code>./infrastructure/config.yaml</code> for your use case</p>"},{"location":"references/config-reference.html#stack-options","title":"Stack Options","text":"<p><code>WORKLOAD_NAME</code></p> <ul> <li> <p>Description: The name of the workload that will deployed. This name will be used as a prefix for for any component deployed into your AWS Account.</p> </li> <li> <p>Type: String </p> </li> <li> <p>Example: <code>\"GameAnalyticsPipeline\"</code></p> </li> </ul>"},{"location":"references/config-reference.html#data-platform-options","title":"Data Platform Options","text":"<p>The following table shows unsupported configurations when options in this section are enabled</p> Control Setting Exception <code>INGEST_MODE</code> <code>DIRECT_BATCH</code> <ul><li><code>DATA_STACK</code> cannot be set to <code>REDSHIFT</code></li><li><code>REAL_TIME_ANALYTICS</code> cannot be set to <code>true</code></li><li>Settings for <code>STREAM_PROVISIONED</code> and <code>STREAM_SHARD_COUNT</code> are ignored since no stream is deployed</li></ul> <code>DATA_STACK</code> <code>REDSHIFT</code> <ul><li><code>ENABLE_APACHE_ICEBERG_SUPPORT</code> cannot be set to <code>true</code></li></ul> <code>REAL_TIME_ANALYTICS</code> <code>true</code> <ul><li><code>INGEST_MODE</code> must be set to <code>KINESIS_DATA_STREAMS</code></li></ul> <p><code>INGEST_MODE</code></p> <ul> <li> <p>Description: Controls the ingestion method for events recieved from the API. When set to <code>\"KINESIS_DATA_STREAMS\"</code> events are ingested into a real-time Kinesis Data Stream for live analytics. When set to <code>\"DIRECT_BATCH\"</code> events are ingested into an Amazon Data Firehose for near-real-time batch ingestion to a data lake.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"KINESIS_DATA_STREAMS\"</code>, <code>\"DIRECT_BATCH\"</code></p> </li> </ul> <p><code>REAL_TIME_ANALYTICS</code></p> <ul> <li> <p>Description: Whether or not to enable the Real-Time component/module of the guidance. It is recommended to set this value to <code>true</code> when first deploying this sample code for testing, as this setting will allow you to verify if streaming analytics is required for your use case. This setting can be changed at a later time, and the guidance re-deployed through CI/CD.</p> </li> <li> <p>Type: Boolean</p> </li> <li> <p>Example: <code>true</code></p> </li> </ul> <p><code>DATA_STACK</code></p> <ul> <li> <p>Description: Controls the data stack that event data is saved to for analysis. When set to <code>\"DATA_LAKE\"</code>, raw events are saved to a data lake in S3 and cataloged using Glue Data Catalog. When set to <code>\"REDSHIFT\"</code> events are using the streaming ingestion feature of Redshift.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"DATA_LAKE\"</code>, <code>\"REDSHIFT\"</code></p> </li> <li> <p>Do not change this configuration after the stack is deployed</p> </li> </ul> <p><code>ENABLE_APACHE_ICEBERG_SUPPORT</code></p> <ul> <li> <p>Description: Whether or not to enable Apache Iceberg support in place of Apache Hive tables. When set to <code>true</code>, the raw events table will be configured as an Apache Iceberg table and the Firehose will be reconfigured to send data as Iceberg transactions. Enabling this option comes with considerations for Firehose.</p> </li> <li> <p>Type: Boolean</p> </li> <li> <p>Example: <code>true</code></p> </li> <li> <p>Do not change this configuration after the stack is deployed. If you would like to enable Iceberg, we recommend deploying a new stack in parallel and migrating existing data.</p> </li> </ul>"},{"location":"references/config-reference.html#real-time-analytics-options","title":"Real-Time Analytics Options","text":"<p>These options are used for when <code>INGEST_MODE</code> is set to <code>KINESIS_DATA_STREAMS</code></p> <p><code>STREAM_PROVISIONED</code></p> <ul> <li>Description: The Kinesis stream capacity mode. When set to <code>true</code>, the stream will be created with the number of shards specified in <code>STREAM_SHARD_COUNT</code>. When set to <code>false</code>, the number of shards will be scaled automatically to handle throughput and the <code>STREAM_SHARD_COUNT</code> setting will be ignored. This value can be changed at a later time and re-deployed through CI/CD. For information about determining the capacity mode required for your use case, refer to Choose the data stream capacity mode in the Amazon Kinesis Data Streams Developer Guide.</li> </ul> <p><code>STREAM_SHARD_COUNT</code></p> <ul> <li> <p>Description: The number of Kinesis shards, or sequence of data records, to use for the data stream. The default value has been set to <code>1</code> for initial deployment, and testing purposes. This value can be changed at a later time, and the guidance re-deployed through CI/CD. For information about determining the shards required for your use case, refer to Amazon Kinesis Data Streams Terminology and Concepts in the Amazon Kinesis Data Streams Developer Guide.</p> </li> <li> <p>Type: Integer</p> </li> <li> <p>Example: <code>1</code></p> </li> <li> <p>Type: Boolean</p> </li> <li> <p>Example: <code>true</code></p> </li> </ul>"},{"location":"references/config-reference.html#data-storage-controls","title":"Data Storage Controls","text":"<p><code>EVENTS_DATABASE</code></p> <ul> <li> <p>Description: Specifies the name of the AWS Glue database that contains the glue tables when <code>DATA_STACK</code> is set to <code>\"DATA_LAKE\"</code>. Specifies the name of the Redshift Serverless database when <code>DATA_STACK</code> is set to <code>\"REDSHIFT\"</code>.</p> </li> <li> <p>Type: String (1-255 characters)</p> </li> <li> <p>Example: <code>\"game_analytics\"</code></p> </li> <li> <p>Limitations: For compatibility with tools, the name should consist of lowercase letters, numbers, and underscores and start with a letter.</p> </li> <li> <p>Do not change this configuration after the stack is deployed</p> </li> </ul> <p><code>RAW_EVENTS_TABLE</code></p> <ul> <li> <p>Description: The name of the of the AWS Glue table within which all new/raw data is cataloged.</p> </li> <li> <p>Type: String (1-255 characters)</p> </li> <li> <p>Example: <code>\"raw_events\"</code></p> </li> <li> <p>Limitations: For compatibility with tools, the name should consist of lowercase letters, numbers, and underscores and start with a letter.</p> </li> <li> <p>Do not change this configuration after the stack is deployed</p> </li> </ul> <p><code>RAW_EVENTS_PREFIX</code></p> <ul> <li> <p>Description: The prefix for new/raw data files stored in S3.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"raw_events\"</code></p> </li> <li> <p>Do not change this configuration after the stack is deployed</p> </li> </ul> <p><code>PROCESSED_EVENTS_PREFIX</code></p> <ul> <li> <p>Description: The prefix processed data files stored in S3.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"processed_events\"</code></p> </li> <li> <p>Do not change this configuration after the stack is deployed</p> </li> </ul> <p><code>GLUE_TMP_PREFIX</code></p> <ul> <li> <p>Description: The name of the temporary data store for AWS Glue.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"glueetl-tmp\"</code></p> </li> </ul>"},{"location":"references/config-reference.html#development-options","title":"Development Options","text":"<p><code>API_STAGE_NAME</code></p> <ul> <li> <p>Description: The name of the REST API stage for the Amazon API Gateway configuration endpoint for sending telemetry data to the pipeline. This provides an integration option for applications that cannot integrate with Amazon Kinesis directly. The API also provides configuration endpoints for admins to use for registering their game applications with the guidance, and generating API keys for developers to use when sending events to the REST API. The default value is set to <code>live</code>.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"live\"</code></p> </li> </ul> <p><code>DEV_MODE</code></p> <ul> <li> <p>Description: Whether or not to enable developer mode. This mode will ensure synthetic data, and shorter retention times are enabled. It is recommended that you set the value to <code>true</code> when first deploying the sample code for testing, as this setting will enable S3 versioning, and won't delete S3 buckets on teardown. This setting can be changed at a later time, and the infrastructure re-deployed through CI/CD.</p> </li> <li> <p>Type: Boolean</p> </li> <li> <p>Example: <code>true</code></p> </li> </ul> <p><code>S3_BACKUP_MODE</code></p> <ul> <li> <p>Description: Whether or not to enable Kinesis Data Firehose to send a backup of new/raw data to S3. The default value has been set to <code>false</code> for initial deployment, and testing purposes. This value can be changed at a later time, and the guidance re-deployed through CI/CD. </p> </li> <li> <p>Type: Boolean</p> </li> <li> <p>Example: <code>false</code></p> </li> </ul>"},{"location":"references/config-reference.html#monitoring-options","title":"Monitoring Options","text":"<p><code>EMAIL_ADDRESS</code></p> <ul> <li> <p>Description: The email address to receive operational notifications, and delivered by CloudWatch.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"user@example.com\"</code></p> </li> </ul> <p><code>CLOUDWATCH_RETENTION_DAYS</code></p> <ul> <li> <p>Description: The default number of days in which Amazon CloudWatch stores all the logs. The default value has been set to <code>30</code> for initial deployment, and testing purposes. This value can be changed at a later time, and the guidance re-deployed through CI/CD. </p> </li> <li> <p>Type: Integer</p> </li> <li> <p>Example: <code>30</code></p> </li> </ul>"},{"location":"references/config-reference.html#version-options","title":"Version Options","text":"<p><code>CDK_VERSION</code></p> <ul> <li> <p>Description: The version of the CDK installed in your environment. To see the current version of the CDK, run the <code>cdk --version</code> command. The guidance has been tested using CDK version <code>2.92.0</code> of the CDK. If you are using a different version of the CDK, ensure that this version is also reflected in the <code>./infrastructure/package.json</code> file.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"2.92.0\"</code></p> </li> </ul> <p><code>NODE_VERSION</code></p> <ul> <li> <p>Description: The version of NodeJS being used. The default value is set to <code>\"latest\"</code>, and should only be changed this if you require a specific version.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"latest\"</code></p> </li> </ul> <p><code>PYTHON_VESION</code></p> <ul> <li> <p>Description: The version of Python being used. The default value is set to <code>\"3.8\"</code>, and should only be changed if you require a specific version.</p> </li> <li> <p>Type: String</p> </li> <li> <p>Example: <code>\"3.8\"</code></p> </li> </ul>"},{"location":"references/file-reference.html","title":"File/Folder Reference","text":""},{"location":"references/file-reference.html#root-file-structure","title":"Root File Structure","text":"<p>This page highlights key components of the file hierarchy when navigating the guidance. It may not include extraneous files for doc clarity.</p> <pre><code>root/\n\u251c\u2500 docs/\n\u251c\u2500 business-logic/\n\u251c\u2500 infrastructure/\n\u251c\u2500 package.json\n</code></pre> <ul> <li> <p>Docs</p> <ul> <li>This folder holds all the files for this documentation site (mkdocs)</li> </ul> </li> <li> <p>Business Logic</p> <ul> <li>This folder holds all of the \"business logic\", non-infrastructure files, such as:<ul> <li>Source scripts and files for Lambda, Glue, Opensearch, API Gateway, Flink to be used for deployment</li> <li>Sample data event generator scripts</li> </ul> </li> </ul> </li> <li> <p>Infrastructure</p> <ul> <li>This folder holds all Infrastructure-as-Code (IaC) deployment files for the guidance. Can choose between AWS CDK and Terraform</li> </ul> </li> <li> <p>Package.json</p> <ul> <li>Has config option for deploying in CDK or Terraform (\"config\": \"iac\": \"cdk\" | \"tf\") and all build commands. See Getting Started for more details</li> </ul> </li> </ul>"},{"location":"references/file-reference.html#business-logic-folder-deep-dive","title":"Business Logic Folder Deep Dive","text":"<pre><code>business-logic/\n\u251c\u2500 api/\n\u2502  \u251c\u2500 admin/\n\u2502  \u251c\u2500 api-definitions/\n\u2502  \u251c\u2500 lambda-authorizer/\n\u251c\u2500 data-lake/\n\u2502  \u251c\u2500 glue-scripts/\n\u251c\u2500 events-processing/\n\u251c\u2500 flink-event-processing/\n\u251c\u2500 opensearch-ingestion/\n\u251c\u2500 publish-data/\n</code></pre> <ul> <li> <p>api</p> <ul> <li>This folder holds the business logic for the API backend:<ul> <li><code>admin/</code> holds all of the API serverless backend logic on AWS Lambda</li> <li><code>api-definitions/</code> holds OpenAPI templates that represent the API calls. Is either a reference for CDK, or directly used as template for deployment for Terraform.</li> <li><code>lambda-authorizer/</code> holds the Lambda code for the component in API Gateway that authorizes API calls before they pass through. See Component Deep Dive for more details</li> </ul> </li> </ul> </li> <li> <p>data-lake</p> <ul> <li>This folder holds all of the ETL scripts that Glue runs as jobs, such as conversion jobs from one format to another, or processing jobs that optimize formats, partition, and send from raw to processed events. The guidance comes with skeleton sample scripts that have inherited best practices, but no direct transformations on the data, since those are customized by you for your use cases</li> </ul> </li> <li> <p>events-processing</p> <ul> <li>Only used during <code>Data Lake</code> mode, Amazon Data Firehose will use the logic in here in an integrated Lambda feature for in-flight data sanitation and ETL option, akin to a \"pre-processing\" option for customers. See Component Deep Dive for details on the pre-processing performed by default. This is different than the ETL processing jobs ran by Glue, which are preferable for cost reasons if data could be processed after ingestion to the S3 data lake, but for business/technical requirements that require processing as part of ingestion, the logic here can be modified</li> </ul> </li> <li> <p>flink-event-processing</p> <ul> <li>Only used if <code>Real Time Analytics</code> is enabled, this folder holds the business logic used by Amazon Managed Service for Apache Flink for real-time processing of events. The sample application code uses Apache Maven and by default has sample transformations based on sample data sent by the <code>publish-data</code> script (see below)</li> </ul> </li> <li> <p>opensearch-ingestion</p> <ul> <li>Only used if <code>Real Time Analytics</code> is enabled, this folder holds the template files used by Amazon Managed Opensearch Service during deployment</li> </ul> </li> <li> <p>publish-data</p> <ul> <li>Has a sample data event generator script on Python that simulates generic game events. The sample Athena queries and Flink sample code are tied to these sample game events, so they will need to be tailored to meet your specific event data</li> </ul> </li> </ul>"},{"location":"references/file-reference.html#infrastructure-folder-deep-dive","title":"Infrastructure Folder Deep Dive","text":"<pre><code>infrastructure/\n\u251c\u2500 aws-cdk/\n\u2502  \u251c\u2500 src/\n\u251c\u2500 terraform/\n\u2502  \u251c\u2500 src/\n\u251c\u2500 config.yaml.TEMPLATE\n</code></pre> <ul> <li> <p>aws-cdk / terraform</p> <ul> <li>Holds the IaC template code for the respective tool, main source files are in <code>/src</code></li> </ul> </li> <li> <p>config.yaml.TEMPLATE</p> <ul> <li>Sample template file for settings/configurations for your deployment, which would be copied into <code>config.yaml</code>. See Getting Started for setting up the config file and deployment.</li> </ul> </li> </ul>"},{"location":"references/file-reference.html#cdk-folder-deep-dive","title":"CDK Folder Deep Dive","text":"<pre><code>src/\n\u251c\u2500 constructs/\n\u2502  \u251c\u2500 samples/\n\u251c\u2500 helpers/\n\u251c\u2500 app-stack.ts\n\u251c\u2500 app.ts\n</code></pre>"},{"location":"references/file-reference.html#terraform-folder-deep-dive","title":"Terraform Folder Deep Dive","text":"<pre><code>\u251c\u2500 src/\n\u251c\u2500 constructs/\n\u2502  \u251c\u2500 samples/\n\u251c\u2500 main.tf\n</code></pre> <ul> <li> <p>constructs</p> <ul> <li>Holds all infrastructure template components for the guidance, but broken down into logical parts called constructs. See below for references for each construct.</li> </ul> </li> <li> <p>(cdk only) app.ts</p> <ul> <li>Contains variable validation and top-level deployment dependencies specific to CDK / Typescript</li> </ul> </li> </ul>"},{"location":"references/file-reference.html#construct-reference","title":"Construct Reference","text":"<ul> <li> <p>main construct (app-stack.ts / main.tf)</p> <ul> <li>Root template file that connects all constructs together along with deploying central components that have dependencies across multiple constructs (Logs + Analytics S3 buckets, DynamoDB tables, SNS Encryption key + IAM policies)</li> </ul> </li> <li> <p>api-construct</p> <ul> <li>Deploys API backend related infrastructure, such as API Gateway and IAM roles</li> </ul> </li> </ul> <ul> <li>dashboard-construct<ul> <li>Deploys the operational real-time CloudWatch dashboard components, such as the dashboard, widgets, and metrics</li> </ul> </li> </ul> <ul> <li>data-lake-construct<ul> <li>Deploys <code>Data Lake Mode</code> components, such as Glue Database, Glue Tables (Iceberg or Hive), and Athena workgroup</li> </ul> </li> </ul> <ul> <li>data-processing-construct<ul> <li>Deploys <code>Data Lake Mode</code> components specifically to ETL processing, such as Glue Workflow, Glue Crawler + Trigger, and Glue ETL Jobs + IAM Roles</li> </ul> </li> </ul> <ul> <li>flink-construct<ul> <li>Deploys <code>Real Time Mode</code> component for Managed Service for Apache Flink, such as the Flink Application + IAM Roles and Flink Log Groups</li> </ul> </li> </ul> <ul> <li>lambda-construct<ul> <li>Deploys all Lambda function components across the guidance, such as <code>Data Lake Mode</code> event processing (Firehose), API Authorizer, API Backend (Admin Service, Redshift deployment service), and corresponding Lambda IAM Roles</li> </ul> </li> </ul> <ul> <li>metrics-construct<ul> <li>Deploys Cloudwatch Alarms such as Kinesis Data Streams alarms if those components are enabled, <code>Data Lake Mode</code> Lambda error alarms, <code>Data Lake Mode</code> Firehose alarms, DynamoDB error alarms, and API Gateway error alarms.</li> </ul> </li> </ul> <ul> <li>opensearch-construct<ul> <li>Deploys Opensearch components such as the serverless cluster, ingestion dead letter queue, encryption, IAM roles, and dashboard </li> </ul> </li> </ul> <ul> <li>redshift-construct<ul> <li>Deploys <code>Redshift Mode</code> components such as Serverless Redshift cluster, encryption, IAM Roles, Security Group added to VPC Construct's networking components</li> </ul> </li> </ul> <ul> <li>streaming-ingestion-construct<ul> <li>Deploys <code>Data Lake Mode</code> Firehose components + IAM role + Log Groups for Iceberg and Hive</li> </ul> </li> </ul> <ul> <li>vpc-construct<ul> <li>Deploys bare-bones VPC components for <code>Redshift Mode</code> or <code>Real Time : Enabled</code> which have dependencies</li> </ul> </li> </ul> <ul> <li>samples/athena-construct<ul> <li>Deploys sample Athena queries in a nested <code>samples</code> folder, which are entirely dependent on the sample event code and is subject to greatest adjustment when implementing your own custom events</li> </ul> </li> </ul>"},{"location":"references/ops-dashboard-reference.html","title":"Operational Dashboard Reference","text":"<p>The Operational Dashboard is a CloudWatch dashboard used to monitor the health of the deployed game analytics pipeline. The contents of the dashboard can change based on the configurations of the deployed pipeline specified in the config.yaml file. For more information about CloudWatch dashboards, please refer to the Amazon CloudWatch User Guide.</p>"},{"location":"references/ops-dashboard-reference.html#operational-health","title":"Operational Health","text":""},{"location":"references/ops-dashboard-reference.html#events-ingestion-and-delivery","title":"Events Ingestion and Delivery","text":"<p>This widget monitors the total number of REST API Calls to ingest events into the game analytics pipeline within the time period. Each REST API call may contain multiple events per payload. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</p>"},{"location":"references/ops-dashboard-reference.html#kinesis-data-stream-ingestion","title":"Kinesis Data Stream Ingestion","text":""},{"location":"references/ops-dashboard-reference.html#events-ingestion-and-delivery_1","title":"Events Ingestion and Delivery","text":"<p>This widget monitors the total number of individual events sent to the real-time kinesis data stream within the time period. The time period for this metric is set to automatic based on the period configured for the entire dashboard. This widget is enabled only if <code>INGEST_MODE</code> is set to <code>KINESIS_DATA_STREAMS</code>.</p>"},{"location":"references/ops-dashboard-reference.html#events-stream-latency","title":"Events Stream Latency","text":"<p>This widget monitors the latency of the real-time raw event data stream. This widget is enabled only if <code>INGEST_MODE</code> is set to <code>KINESIS_DATA_STREAMS</code>.</p> <ul> <li>PutRecords Write Latency - this measures the average latency in milliseconds of the PutRecords operation (multi record insert) within the time period. This metric is used to monitor the latency of inserts into the pipeline. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>Read Latency - this measures the average latency of a read operation in milliseconds from the Kinesis Data Stream within the time period. This metric is used to monitor the latency of a read request by the consumers of raw events (Amazon Data Firehose, Amazon Managed Service for Apache Flink, Amazon Redshift). The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>Consumer Iterator Age - this tracks the average read position of the data being processed by a downstream consumer (Amazon Data Firehose, Managed Service for Apache Flink app). If the iterator age passes 50% of the configured stream retention period, there is a risk for data loss. This metric can occasionally spike if the pipeline is idle. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>PutRecord Write Latency - this measures the average latency of the PutRecord operation (single record insert) within the time period. This metric is often not published as the API Gateway batches events using the PutRecords API by default. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> </ul> <p>You can read more about Kinesis stream-level metrics in the Amazon Kinesis Data Streams Developer Guide and troubleshooting using the metrics in the troubleshooting documentation.</p>"},{"location":"references/ops-dashboard-reference.html#redshift-serverless","title":"Redshift Serverless","text":""},{"location":"references/ops-dashboard-reference.html#queries-completed-per-second","title":"Queries Completed Per Second","text":"<p>This widget tracks the number of queries completed per second.</p> <p>For more information about the metrics that Redshift Serverless tracks, please refer to the Amazon Redshift Management Guide.</p>"},{"location":"references/ops-dashboard-reference.html#database-connections","title":"Database Connections","text":"<p>This widget tracks the number of open connections to the Redshift database at a point in time.</p> <p>For more information about the metrics that Redshift Serverless tracks, please refer to the Amazon Redshift Management Guide.</p>"},{"location":"references/ops-dashboard-reference.html#query-planning-execution","title":"Query Planning / Execution","text":"<p>This widget tracks the time spent planning and executing queries on the event data.</p> <ul> <li>QueryPlanning - This metric tracks the time spent parsing and optimizing SQL statements.</li> <li>QueryExecutingRead - This metric tracks the time spent executing read queries.</li> </ul> <p>For more information about the metrics that Redshift Serverless tracks, please refer to the Amazon Redshift Management Guide.</p>"},{"location":"references/ops-dashboard-reference.html#data-storage","title":"Data Storage","text":"<p>This widget tracks the amount of data stored in the Redshift serverless data.</p> <p>For more information about the metrics that Redshift Serverless tracks, please refer to the Amazon Redshift Management Guide.</p>"},{"location":"references/ops-dashboard-reference.html#stream-ingestion-processing","title":"Stream Ingestion &amp; Processing","text":""},{"location":"references/ops-dashboard-reference.html#events-processing-health","title":"Events Processing Health","text":"<p>This widget monitors the firehose delivery stream used to deliver raw events to the data lake.</p> <ul> <li>Data Freshness - This metric monitors the average age of the oldest record in Amazon Data Firehose that has not been delivered into Amazon S3 within the time period. The age is defined as the time difference from ingestion until current time. Any records with an age higher than this metric have been delivered into S3. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>Firehose Records Delivered to S3 - This measures the average number of game events that have been delivered into Amazon S3 within the time period. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>Lambda Duration - This measures the average amount of time that the lambda spends processing an event for data transformation using Amazon Data Firehose within the time period. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>Lambda Concurrency - This measures the highest number of concurrent lambdas executing for data transformation using Amazon Data Firehose within the time period. The time period for this metric is set to automatic based on the period configured for the entire dashboard. If this metric exceeds the quota for the region or the reserved concurrency limit for the function, requests will become throttled.</li> <li>Lambda Throttles - This measures the total number of throttled requests to the data transformation lambda within the time period. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> </ul> <p>You can view more about the metrics emitted by Amazon Data Firehose in the Amazon Data Firehose Developer Guide.</p>"},{"location":"references/ops-dashboard-reference.html#event-transformation-lambda-error-count-and-success-rate","title":"Event Transformation Lambda Error count and success rate (%)","text":"<p>This widget monitors the activity on the transformation lambda function used in Amazon Data Firehose.</p> <ul> <li>Errors - This metric tracks the total number of invocations resulting in function errors within the time period.</li> <li>Invocations - This metric tracks the total number of function invocations within the time period.</li> <li>Success Rate (%) - This metric tracks the percentage of lambda invocations that were successful within the time period. This metric is tracked on the right axis of the graph as it measures percentage instead of count.</li> </ul> <p>You can view more about Lambda function metrics in the AWS Lambda Developer Guide.</p>"},{"location":"references/ops-dashboard-reference.html#real-time-streaming-analytics","title":"Real-time Streaming Analytics","text":""},{"location":"references/ops-dashboard-reference.html#managed-flink-records-intake","title":"Managed Flink Records Intake","text":"<p>This widget monitors the number of records that the Managed Service for Apache Flink application is recieving from the input stream. This metric is only enabled when <code>REAL_TIME_ANALYTICS</code> is set to <code>true</code>.</p> <ul> <li>Number of Records Recieved - This metric tracks total number of records that the Managed Service for Apache Flink application has recieved within the time window.</li> <li>Number of Late Records Dropped - This metric tracks total number of records that the  Managed Service for Apache Flink application has dropped due to arriving late.</li> </ul> <p>For more information about the metrics the Managed Service For Apache Flink tracks, please refer to the developer guide.</p>"},{"location":"references/ops-dashboard-reference.html#managed-flink-container-cpu-utilization","title":"Managed Flink Container CPU Utilization","text":"<p>This widget monitors the container CPU utilization of the Managed Service for Apache Flink application. Container CPU utilization is the metric used to determine auto-scaling actions for the Managed Service for Apache Flink application which are annotated on the graph. This metric is only enabled when <code>REAL_TIME_ANALYTICS</code> is set to <code>true</code>.</p> <p>If this metric is above the Scale Up Threshold for 15 minutes or more, an auto-scaling action will be triggered to double the parallelism of the application. If this metric is below the Scale Down Threshold for six hours or more, an auto-scaling action will be triggered to halve the parallelism of the application. You can read more about Flink autoscaling in the Developer Guide for Managed Service for Apache Flink. </p>"},{"location":"references/ops-dashboard-reference.html#managed-flink-container-resource-utilization","title":"Managed Flink Container Resource Utilization","text":"<p>This widget monitors resource utilization of the Managed Service for Apache Flink application. These resources are not used for auto-scaling, but are important to monitor for performance regressions. This metric is only enabled when <code>REAL_TIME_ANALYTICS</code> is set to <code>true</code>.</p> <ul> <li>threadsCount - The total number of live threads used by the application.  </li> <li>containerMemoryUtilization - Overall percentage of memory utilization across task manager containers in Flink application cluster.</li> <li>containerDiskUtilization - Overall percentage of disk utilization across task manager containers in Flink application cluster.</li> </ul> <p>For more information about the metrics the Managed Service For Apache Flink tracks, please refer to the developer guide.</p>"},{"location":"references/ops-dashboard-reference.html#opensearch-intake","title":"OpenSearch Intake","text":"<p>This widget monitors the flow of custom metric documents that are ingested into OpenSearch Serverless. The left side of the widget monitors successful records ingested, while the right side of the widget monitors documents that errored out during ingestion. This metric is only enabled when <code>REAL_TIME_ANALYTICS</code> is set to <code>true</code>.</p> <ul> <li>Collection Ingested - The number of records at which documents are being ingested to the OpenSearch Serverless Collection.</li> <li>Pipeline Recieved - The number of records at which documents are being ingested into the OpenSearch Ingestion pipeline.</li> <li>Pipeline Sent - The number of documents successfully sent from the OpenSearch Ingestion pipeline to the OpenSearch Collection.</li> <li>Pipeline Errors - The number of documents that failed to be sent from the OpenSearch Ingestion pipeline.</li> <li>Collection Errors - The total number of document errors during ingestion into the OpenSearch Serverless Collection. After a successful bulk indexing request, writers process the request and emit errors for all failed documents within the request.</li> </ul> <p>For more information about the metrics emitted by Amazon OpenSearch Serverless, refer to the developer guide. For more information about the metrics emitted by Amazon OpenSearch Ingestion, refer to the developer guide on pipeline metrics.</p>"},{"location":"references/ops-dashboard-reference.html#metrics-stream-latency","title":"Metrics Stream Latency","text":"<p>This widget monitors the latency of the custom metrics stream. The custom metrics stream contains metrics created by the Managed Service for Apache Flink Application in flight to OpenSearch. This metric is only enabled when <code>REAL_TIME_ANALYTICS</code> is set to <code>true</code>.</p> <ul> <li>PutRecords Write Latency - this measures the average latency in milliseconds of the PutRecords operation (multi record insert) within the time period. This metric is used to monitor the latency of inserts into the pipeline. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>Read Latency - this measures the average latency of a read operation in milliseconds from the Kinesis Data Stream within the time period. This metric is used to monitor the latency of a read request by the consumers of raw events (Amazon Data Firehose, Amazon Managed Service for Apache Flink, Amazon Redshift). The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>Consumer Iterator Age - this tracks the average read position of the data being processed by a downstream consumer (Amazon Data Firehose, Managed Service for Apache Flink app). If the iterator age passes 50% of the configured stream retention period, there is a risk for data loss. This metric can occasionally spike if the pipeline is idle. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> <li>PutRecord Write Latency - this measures the average latency of the PutRecord operation (single record insert) within the time period. This metric is often not published as the Flink connector batches events using the PutRecords API by default. The time period for this metric is set to automatic based on the period configured for the entire dashboard.</li> </ul> <p>You can read more about Kinesis stream-level metrics in the Amazon Kinesis Data Streams Developer Guide and troubleshooting using the metrics in the troubleshooting documentation.</p>"},{"location":"references/ops-dashboard-reference.html#opensearch-latency","title":"OpenSearch Latency","text":"<p>This widget monitors the end-to-end latency of the system used to ingest custom metrics from the custom metrics stream into OpenSearch. This metric is only enabled when <code>REAL_TIME_ANALYTICS</code> is set to <code>true</code>.</p> <ul> <li>Collection Ingestion Request Latency - The average latency for bulk write operations to the Amazon OpenSearch Serverless collection within the time period.</li> <li>Pipeline End-To-End Latency - The average end-to-end latency of the Amazon OpenSearch Ingestion pipeline. This measures the total latency of the ingestion pipeline as it recieves and buffers events before delivering them to the collection. If this metric is high, consider increasing the OCU limit of the pipeline to increase parallelism.</li> </ul> <p>For more information about the metrics emitted by Amazon OpenSearch Serverless, refer to the developer guide. For more information about the metrics emitted by Amazon OpenSearch Ingestion, refer to the developer guide on pipeline metrics.</p>"},{"location":"references/output-reference.html","title":"Output Reference","text":"<p>This page explains the outputs displayed by the stack after a successful deployment. Due to differences in naming convention, the format of the outputs differs between AWS Cloud Development Kit and Hashicorp Terraform, however, the meaning of each output is consistent across both versions.</p>"},{"location":"references/output-reference.html#admin-api-access-policy-name","title":"Admin API Access Policy Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.AdminApiAccessPolicyName</code></li> <li>Terraform Output: <code>admin_api_access_policy_name</code></li> <li>Description: The name of the IAM Managed Policy that will allow an IAM entity to execute the Admin API</li> </ul>"},{"location":"references/output-reference.html#analytics-bucket-name","title":"Analytics Bucket Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.AnalyticsBucketName</code></li> <li>Terraform Output: <code>analytics_bucket_name</code></li> <li>Description: The name of the S3 Bucket used for game analytics storage</li> </ul>"},{"location":"references/output-reference.html#api-endpoint","title":"API Endpoint","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.ApiEndpoint</code></li> <li>Terraform Output: <code>api_endpoint</code></li> <li>Description: The base URL of the Game Analytics API. This is the endpoint used to perform administration actions and recieve events</li> </ul>"},{"location":"references/output-reference.html#api-gateway-execution-logs-link","title":"API Gateway Execution Logs Link","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.ApiGatewayExecutionLogsLink</code></li> <li>Terraform Output: <code>api_gateway_execution_logs_link</code></li> <li>Description: A web link to the CloudWatch logs emitted from API Gateway</li> </ul>"},{"location":"references/output-reference.html#applications-table-name","title":"Applications Table Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.ApplicationsTableName</code></li> <li>Terraform Output: <code>applications_table_name</code></li> <li>Description: The name of the DynamoDB configuration table that stores information about the registered applications allowed by the solution pipeline</li> </ul>"},{"location":"references/output-reference.html#flink-app-name","title":"Flink App Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.FlinkAppName</code></li> <li>Terraform Output: <code>flink_app_name</code></li> <li>Description: The name of the Amazon Managed Service for Apache Flink application. This is only enabled when REAL_TIME_ANALYTICS is set to <code>true</code>.</li> </ul>"},{"location":"references/output-reference.html#game-events-database-name","title":"Game Events Database Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.GameEventsDatabaseName</code></li> <li>Terraform Output: <code>game_events_database_name</code></li> <li>Description: The name of the Glue Data Catalog database where game events are stored. This is only enabled when DATA_STACK is set to <code>\"DATA_LAKE\"</code>.</li> </ul>"},{"location":"references/output-reference.html#game-events-etl-job-name","title":"Game Events ETL Job Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.GameEventsEtlJobName</code></li> <li>Terraform Output: <code>game_events_etl_job_name</code></li> <li>Description: The name of the ETL job used to move data from the raw events table to the processed events table. This is only enabled when DATA_STACK is set to <code>\"DATA_LAKE\"</code>.</li> </ul>"},{"location":"references/output-reference.html#game-events-etl-iceberg-job-name","title":"Game Events ETL Iceberg Job Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.GameEventsIcebergJobName</code></li> <li>Terraform Output: <code>game_events_etl_iceberg_job_name</code></li> <li>Description: The name of the ETL job used to move data from an existing Game Analytics Pipeline Hive table to a new Apache Iceberg table. This is only enabled when DATA_STACK is set to <code>\"DATA_LAKE\"</code> and when ENABLE_APACHE_ICEBERG_SUPPORT is set to <code>true</code>.</li> </ul>"},{"location":"references/output-reference.html#game-events-stream-name","title":"Game Events Stream Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.GameEventsStreamName</code></li> <li>Terraform Output: <code>game_events_stream_name</code></li> <li>Description: The name of the Kinesis Data Stream for ingestion of raw events. This is only enabled when INGEST_MODE is set to <code>\"KINESIS_DATA_STREAMS\"</code>.</li> </ul>"},{"location":"references/output-reference.html#glue-workflow-console-link","title":"Glue Workflow Console Link","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.GlueWorkflowConsoleLink</code></li> <li>Terraform Output: <code>glue_workflow_console_link</code></li> <li>Description: A web link to the AWS Glue Workflows console page to view details about the deployed workflow</li> </ul>"},{"location":"references/output-reference.html#iceberg-setup-job-name","title":"Iceberg Setup Job Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.IcebergSetupJobName</code></li> <li>Terraform Output: <code>iceberg_setup_job_name</code></li> <li>Description: The name of the Glue Job used to configure partitioning on a newly created Apache Iceberg table. This is only enabled when DATA_STACK is set to <code>\"DATA_LAKE\"</code>.</li> </ul>"},{"location":"references/output-reference.html#metric-output-stream-name","title":"Metric Output Stream Name","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.MetricOutputStreamName</code></li> <li>Terraform Output: <code>metric_output_stream_name</code></li> <li>Description: The name of the intermediary Amazon Kinesis Data Stream between Managed Service for Apache Flink and OpenSearch Ingestion. This is only enabled when REAL_TIME_ANALYTICS is set to <code>true</code>.</li> </ul>"},{"location":"references/output-reference.html#opensearch-admin-assume-link","title":"OpenSearch Admin Assume Link","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.OpenSearchAdminAssumeLink</code></li> <li>Terraform Output: <code>opensearch_admin_assume_link</code></li> <li>Description: Link to assume the role of an OpenSearch admin.  This is only enabled when REAL_TIME_ANALYTICS is set to <code>true</code>.</li> </ul>"},{"location":"references/output-reference.html#opensearch-dashboard-link","title":"OpenSearch Dashboard Link","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.OpenSearchDashboardLink</code></li> <li>Terraform Output: <code>opensearch_dashboard_link</code></li> <li>Description: A link to the OpenSearch UI Application to view real-time custom metrics. This is only enabled when REAL_TIME_ANALYTICS is set to <code>true</code>.</li> </ul>"},{"location":"references/output-reference.html#pipeline-operations-dashboard-link","title":"Pipeline Operations Dashboard Link","text":"<ul> <li>CDK Output: <code>CentralizedGameAnalytics.PipelineOperationsDashboardLink</code></li> <li>Terraform Output: <code>pipeline_operations_dashboard_link</code></li> <li>Description: A web link to the CloudWatch dashboard to monitor the health of the pipeline</li> </ul>"},{"location":"references/schema-reference.html","title":"Schema Reference","text":"<p>The solution configures the following JSON schema to validate telemetry data sent to the solution API. To see how the solution utilizes the JSON schema during the event process, see Component Deep Dive for details on the process.</p> <p>Info</p> <p>The <code>application_id</code> field is not required when sending events using the solution API events endpoint because it is automatically set in the data record by the API using the path of the API request after the request has been authorized. Applications that integrate directly with Amazon Kinesis Data Streams must provide an <code>application_id</code> for each event that is submitted.</p> <p>There are sample queries built into the solution that you can refer to in Customizations. There are also sample scripts that create sample events built into the solution that you can utilize and refer to in Getting Started.</p>"},{"location":"references/schema-reference.html#event-schema-sample","title":"Event Schema Sample","text":"<pre><code>{\n    \"event_id\": \"34c74de5-69d9-4f06-86ac-4b98fef8bca9\",\n    \"event_name\": \"login\",\n    \"event_type\": \"client\",\n    \"event_version\": \"1.0.0\",\n    \"event_timestamp\": 1737658977,\n    \"app_version\": \"1.0.0\",\n    \"event_data\":\n    {\n        \"platform\": \"pc\",\n        \"last_login_time\": 1737658477\n    }\n}\n</code></pre>"},{"location":"references/schema-reference.html#definitions","title":"Definitions","text":""},{"location":"references/schema-reference.html#event_id","title":"<code>event_id</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Description:<ul> <li>A random UUID that unique identifies this event. Your event sources should handle logic that creates the UUID for each event. This is a best practice to allow tracking down of individual events for further analysis or diagnosis.</li> </ul> </li> </ul>"},{"location":"references/schema-reference.html#event_name","title":"<code>event_name</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Description:<ul> <li>An identifier for the event that identifies what kind of event is being passed in.</li> </ul> </li> </ul>"},{"location":"references/schema-reference.html#event_type","title":"<code>event_type</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Description:<ul> <li>A grouping for event names that allows categorization of common events within a type.</li> </ul> </li> </ul>"},{"location":"references/schema-reference.html#event_version","title":"<code>event_version</code>","text":"<ul> <li>Type: <code>string</code></li> <li>Description:<ul> <li>The version of the event's schema, allows organizing events by their version as the schema evolves over time.</li> </ul> </li> </ul>"},{"location":"references/schema-reference.html#event_timestamp","title":"<code>event_timestamp</code>","text":"<ul> <li>Type: <code>number</code></li> <li>Description:<ul> <li>The time in seconds since the Unix epoch at which this event occurred, set by the producer of event.</li> </ul> </li> </ul>"},{"location":"references/schema-reference.html#app_version","title":"<code>app_version</code>","text":"<ul> <li>Type: <code>number</code></li> <li>Description:<ul> <li>The version of the application/game, allows organizing events by the application/game's version as it updates over time.</li> </ul> </li> </ul>"},{"location":"references/schema-reference.html#event_data","title":"<code>event_data</code>","text":"<ul> <li>Type: <code>json</code></li> <li>Description:<ul> <li>Nested json blob that contains the event's specific schema values. The above top level schema reflects values that all events should have, while the values in <code>event_data</code> are specific to the event.</li> </ul> </li> </ul>"},{"location":"upgrading/v2-migration.html","title":"Upgrading Guidance for Game Analytics Pipeline to Version 3","text":"<p>This implementation guide outlines the steps required to migrate from V2 to V3 of the Game Analytics Pipeline on AWS, with a specific focus on the data migration from Parquet to Apache Iceberg format.</p>"},{"location":"upgrading/v2-migration.html#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Key Differences Between V2 and V3</li> <li>Prerequisites</li> <li>Data Migration Considerations: Parquet to Iceberg</li> <li>Migration Steps</li> <li>Post-Migration Validation</li> <li>Troubleshooting </li> </ol>"},{"location":"upgrading/v2-migration.html#overview","title":"Overview","text":"<p>The Game Analytics Pipeline on AWS has evolved from V2 to V3, with one of the most significant changes being the migration from Parquet to Apache Iceberg as the data storage format. This guide provides a comprehensive approach to implementing this migration while ensuring data integrity and minimal disruption to analytics workflows.</p>"},{"location":"upgrading/v2-migration.html#key-differences-between-v2-and-v3","title":"Key Differences Between V2 and V3","text":""},{"location":"upgrading/v2-migration.html#managed-service-for-apache-flink","title":"Managed Service for Apache Flink","text":"<p>Amazon Kinesis Data Analytics for SQL applications, which is used in the original architecture of the Game Analytics Pipeline on AWS, is being discontinued. You can learn more about the discontinuation using the following resources.</p> <ul> <li>Amazon Kinesis Data Analytics for SQL Applications discontinuation</li> </ul> <p>Amazon Managed Service for Apache Flink is a serverless, low-latency, highly scalable, and highly available real-time stream processing service. Amazon Managed Service for Apache Flink replaces Amazon Kinesis Data Analytics for SQL Applications in the Game Analytics Pipeline on AWS architecture.</p> <p>Amazon Web Services has published a guide and examples to assist with the migration to Flink. The resources are accessible using the links below.</p> <ul> <li>Migrate from Amazon Kinesis Data Analytics for SQL to Amazon Managed Service for Apache Flink and Amazon Managed Service for Apache Flink Studio</li> <li>Migrating to Managed Service for Apache Flink Studio Examples</li> </ul>"},{"location":"upgrading/v2-migration.html#what-is-apache-iceberg","title":"What is Apache Iceberg?","text":"<p>Apache Iceberg is an Open Table Format (OTF) for large analytic datasets that provides significant advantages over traditional file formats like Parquet when used alone. It maintains tables through metadata that tracks  all data files within a table, enabling schema evolution, hidden partitioning, time travel capabilities, and ACID transactions. Unlike traditional Hive-style partitioning, Iceberg uses a high-performance format that handles partition evolution and complex types while supporting full SQL queries. It was designed to solve performance and reliability issues in large tables, offering improved query planning, reliable writes with atomic commits, and concurrent reads during writes. Iceberg works with various processing engines including Spark, Trino, PrestoDB, Flink, and Hive, making it a versatile choice for modern data lake architectures. Its ability to provide snapshot isolation, schema evolution without table rewrites, and partition evolution without data migration makes it particularly valuable for organizations managing large-scale analytics workloads that require both flexibility and performance.</p>"},{"location":"upgrading/v2-migration.html#features","title":"Features","text":"<ul> <li>Schema evolution</li> <li>Hidden partitioning</li> <li>Time travel capabilities</li> <li>ACID transactions</li> <li>Improved query performance</li> <li>Compatibility with various processing engines</li> </ul> Feature V2 (Parquet) V3 (Iceberg) Data Format Parquet files Apache Iceberg (Iceberg managed parquet using Amazon S3 as underlying storage) Schema Evolution Limited, requires table recreation and schema validations Seamless schema evolution Time Travel Not supported Supported (query data at specific points in time) Transactions Not supported Supported Partitioning Explicit partitioning Hidden partitioning Query Performance Good Improved with metadata and partitioning pruning Configuration Simple Requires iceberg-specific settings AWS Glue Integration Native Requires Glue 3.0+ Amazon Data Firehose Native Native"},{"location":"upgrading/v2-migration.html#prerequisites","title":"Prerequisites","text":"<p>Before beginning the migration, ensure you have:</p>"},{"location":"upgrading/v2-migration.html#knowledge-requirements","title":"Knowledge Requirements","text":"<ul> <li>Familiarity with AWS services and GAP v2</li> <li>Basic understanding of data lake concepts</li> <li>Understanding of Apache Iceberg principles</li> </ul>"},{"location":"upgrading/v2-migration.html#backup","title":"Backup","text":"<p>Full backup of existing Parquet data: You can replicate your data or fully copy the Amazon S3 bucket in order to assure that your existing data is secure and can be recovered for roll-back purposes.</p>"},{"location":"upgrading/v2-migration.html#migration-steps","title":"Migration Steps","text":"<p>Data Backup for existing events from V2 using AWS CLI for Amazon S3 to Amazon S3 Copy</p> <p>This is the most straightforward approach for a one-time backup but feel free to use any back strategy you have implemented:</p> <pre><code># Sync all Parquet files from source to backup mantaining the entire data structure \naws s3 sync s3://source-analytics-bucket/ s3://backup-bucket/parquet-backup/\n</code></pre> <p>Prepare the Environment and Update Configuration. </p> <p>For migration purposes is required to have both versions installed during the migration from parquet to iceberg. Follow the deployment instructions in the manual (link). You have to enable Iceberg support in the configuration by setting <code>ENABLE_APACHE_ICEBERG_SUPPORT: true</code> in your configuration file.</p> <pre><code>ENABLE_APACHE_ICEBERG_SUPPORT: true,\n</code></pre> <p>Data Migration: Parquet to Iceberg</p> <p>There are two easy ways to migrate your data depending on the current AWS Analytics services stack you are using:</p>"},{"location":"upgrading/v2-migration.html#amazon-athena","title":"Amazon Athena","text":"<pre><code>INSERT INTO \"database\".\"raw_events_table_v3\" SELECT * FROM \"database\".\"raw_events_table_v2\"\n</code></pre> <p>You can use the same for Apache Iceberg table:</p> <pre><code>INSERT INTO \"database\".\"iceberg_table\"\nSELECT event_id, event_type, event_name, event_version, event_timestamp, app_version, application_id, application_name, event_data, metadata \nFROM \"database\".\"raw_events_table\"\n</code></pre>"},{"location":"upgrading/v2-migration.html#aws-glue","title":"AWS Glue","text":"<p>The migration from Parquet to Iceberg involves converting existing data and updating table definitions. The V2 pipeline includes a Glue job (<code>convert_game_events_to_iceberg.py</code>) specifically for this purpose.</p> <ol> <li> <p>Data Migration Process:</p> <ol> <li>Create Iceberg Tables**: The CDK deployment will create new Iceberg tables in the AWS Glue Data Catalog.</li> <li>Run the Conversion Job**: Execute the Glue job to convert existing Parquet data to Iceberg format pointing to recently created tables.</li> </ol> </li> <li> <p>Key Considerations During Migration:</p> <ol> <li>Partitioning: Apache Iceberg uses hidden partitioning and represents a significant shift from traditional Hive-style partitioning that you might be familiar with in Parquet-based data lakes. When you migrate to Iceberg, one of the most noticeable changes is that you will no longer see the explicit partition folders in your amazon S3 bucket structure. Read more here<ol> <li>V2 (Parquet)**: Explicit partitioning by, <code>year</code>, <code>month</code>, <code>day</code></li> <li>V3 (Iceberg)**: Hidden partitioning managed by Iceberg, for improved query performance</li> </ol> </li> <li>Schema Handling: Apache Iceberg handles schema evolution differently than Parquet. The migration process preserves the schema but the date column is not required for partitioning anymore. Read more here</li> </ol> </li> <li>Rollback Plan:<ol> <li>If issues arise during migration, follow these rollback steps:<ol> <li>Revert to Parquet Tables**:</li> <li>Update application configurations to use original Parquet tables</li> </ol> </li> </ol> </li> <li>Validate data access and integrity</li> </ol>"},{"location":"upgrading/v2-migration.html#post-migration-data-validation","title":"Post-Migration Data Validation","text":"<p>You can create a post-migration validation framework that includes the following key verification methods but not limited to include your own validations: </p> <ul> <li>Record Count Validation: Compares the total number of records between source and target systems to ensure completeness.</li> <li>Data Integrity Checks: Examine critical columns for null values and identify duplicate records that might indicate migration issues.</li> <li>Data Consistency Validation: Compares aggregate values like sums, minimums, maximums, and averages to verify computational accuracy.</li> <li>Sample Record Comparison: Inspects specific records by their identifiers to confirm detailed data fidelity.</li> <li>Data Distribution Check: Analyzes the frequency distribution of categorical data to ensure proportional representation was maintained.</li> <li>Schema Validation: Confirms that column names, data types, and structural elements were correctly transferred.</li> <li>Reconciliation Reports: summarize the validation results including success rates, failed records, data quality, metrics, and performance statistics to provide stakeholders with a comprehensive view of migration success.</li> </ul>"},{"location":"upgrading/v2-migration.html#record-count-validation","title":"Record Count Validation","text":"<pre><code>sql\n-- Parquet table count\nSELECT COUNT(*) AS parquet_count FROM \"database\".\"parquet_table\";\n\n-- Iceberg table count\nSELECT COUNT(*) AS iceberg_count FROM \"database\".\"iceberg_table\";\n</code></pre>"},{"location":"upgrading/v2-migration.html#data-integrity-checks","title":"Data Integrity Checks","text":"<pre><code>sql\n-- Check nulls in Parquet table\nSELECT \n  SUM(CASE WHEN id IS NULL THEN 1 ELSE 0 END) AS null_id,\n  SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS null_customer_id,\n  SUM(CASE WHEN transaction_date IS NULL THEN 1 ELSE 0 END) AS null_transaction_date\nFROM \"database\".\"parquet_table\";\n\n-- Check nulls in Iceberg table\nSELECT \n  SUM(CASE WHEN id IS NULL THEN 1 ELSE 0 END) AS null_id,\n  SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) AS null_customer_id,\n  SUM(CASE WHEN transaction_date IS NULL THEN 1 ELSE 0 END) AS null_transaction_date\nFROM \"database\".\"iceberg_table\";\n</code></pre>"},{"location":"upgrading/v2-migration.html#data-consistency-validation","title":"Data Consistency Validation","text":"<pre><code>sql\n-- Parquet metrics\nSELECT \n  SUM(amount) AS sum_amount,\n  MIN(amount) AS min_amount,\n  MAX(amount) AS max_amount,\n  AVG(amount) AS avg_amount,\n  COUNT(DISTINCT customer_id) AS distinct_customers\nFROM \"database\".\"parquet_table\";\n\n-- Iceberg metrics\nSELECT \n  SUM(amount) AS sum_amount,\n  MIN(amount) AS min_amount,\n  MAX(amount) AS max_amount,\n  AVG(amount) AS avg_amount,\n  COUNT(DISTINCT customer_id) AS distinct_customers\nFROM \"database\".\"iceberg_table\";\n</code></pre>"},{"location":"upgrading/v2-migration.html#sample-record-comparison","title":"Sample Record Comparison","text":"<pre><code>sql\n-- Sample from Parquet\nSELECT * \nFROM \"database\".\"parquet_table\"\nWHERE id IN ('12345', '67890', '24680', '13579', '97531')\nORDER BY id;\n\n-- Sample from Iceberg\nSELECT * \nFROM \"database\".\"iceberg_table\"\nWHERE id IN ('12345', '67890', '24680', '13579', '97531')\nORDER BY id;\n</code></pre>"},{"location":"upgrading/v2-migration.html#data-distribution-check","title":"Data Distribution Check","text":"<pre><code>sql\n-- Parquet distribution\nSELECT \n  status,\n  COUNT(*) AS record_count,\n  ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"database\".\"parquet_table\"), 2) AS percentage\nFROM \"database\".\"parquet_table\"\nGROUP BY status\nORDER BY COUNT(*) DESC;\n\n-- Iceberg distribution\nSELECT \n  status,\n  COUNT(*) AS record_count,\n  ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM \"database\".\"iceberg_table\"), 2) AS percentage\nFROM \"database\".\"iceberg_table\"\nGROUP BY status\nORDER BY COUNT(*) DESC;\n</code></pre>"},{"location":"upgrading/v2-migration.html#schema-validation","title":"Schema Validation","text":"<pre><code>sql\n-- Parquet schema\nSELECT \n  column_name,\n  data_type,\n  ordinal_position\nFROM information_schema.columns\nWHERE table_schema = 'database' \nAND table_name = 'parquet_table'\nORDER BY ordinal_position;\n\n-- Iceberg schema\nSELECT \n  column_name,\n  data_type,\n  ordinal_position\nFROM information_schema.columns\nWHERE table_schema = 'database' \nAND table_name = 'iceberg_table'\nORDER BY ordinal_position;\n</code></pre>"},{"location":"upgrading/v2-to-v3-changes.html","title":"Changes from V2 to V3","text":""},{"location":"upgrading/v2-to-v3-changes.html#as-of-7312025","title":"As of 7/31/2025","text":""},{"location":"upgrading/v2-to-v3-changes.html#feature-updates","title":"Feature Updates","text":"<ul> <li>Added Iceberg table support, CTAS queries for creating Iceberg tables, and Glue scripts for converting Hive to Iceberg</li> <li>Added Opensearch as the new real-time dashboarding over CloudWatch metrics</li> <li>Added Terraform as a deployment option</li> <li>Added Redshift as a deployment option</li> </ul>"},{"location":"upgrading/v2-to-v3-changes.html#infrastructure-updates","title":"Infrastructure Updates","text":"<p>Upgraded infrastructure to the following: (INSERT BEFORE AND AFTERS HERE)</p> <p>Info</p> <p>Explanations can be found in Component Deep Dive and Design Considerations</p> <ul> <li>Replaced Kinesis Data Analytics with Managed Flink</li> <li>Added deployment option for Direct Batching to Firehose</li> <li>Removed Ops-pipeline components (Github Actions, CodeBuild, CodePipeline)</li> <li>Added S3 Tables option for Iceberg table support</li> <li>Lambda functions now run on Graviton</li> </ul>"},{"location":"upgrading/v2-to-v3-changes.html#configuration-updates","title":"Configuration Updates","text":"<ul> <li>Added option/support for <code>ON_DEMAND</code> Kinesis Data Streams through <code>STREAM_PROVISIONED : true/false</code></li> <li>Replaced <code>ENABLE_STREAMING_ANALYTICS</code> with <code>REAL_TIME_ANALYTICS: true | false</code></li> <li>Added <code>INGEST_MODE: \"KINESIS_DATA_STREAMS\" | \"DIRECT_BATCH\"</code> to support future ingest options</li> <li>Added <code>DATA_STACK: \"DATA_LAKE\" | \"REDSHIFT\"</code> to support Redshift deployment option</li> <li>Added default values for configuration template file</li> <li>Added \"iac\" field to top-level package.json to support CDK or Terraform deployment options</li> <li>Reorganized config variables to functional groups</li> </ul>"},{"location":"upgrading/v2-to-v3-changes.html#administrative-updates","title":"Administrative Updates","text":"<ul> <li>Added mkdocs and documentation to the repository</li> <li>Removed Solution Helper and custom resources dependency for CDK</li> <li>Streamlining esbuild as primary deployment option</li> <li>Created dashboard-construct to move CloudWatch Dashboard to a dedicated construct</li> <li>Revamped CloudWatch Dashboard, see Ops Dashboard Reference for latest state</li> <li>Added additional metrics for Flink, Opensearch, and Redshift to Operational Dashboard and dynamically builds based on deployment</li> </ul>"},{"location":"upgrading/v2-to-v3-changes.html#library-updates","title":"Library Updates","text":"<ul> <li>Updated CDK version for repo</li> <li>Updated AWS SDK to v3</li> <li>Updated NPM libraries</li> <li>Updated Lambda libraries to Node v22</li> <li>Updated Glue Engine to 5.0</li> <li>Updated GlueParquet and Python UTC functions for Glue Script</li> <li>Now requires Maven and Terraform as library requirements</li> </ul>"}]}